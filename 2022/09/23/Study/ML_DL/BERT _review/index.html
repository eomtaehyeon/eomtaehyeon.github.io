<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />

    

    
    <title>ë…¼ë¬¸ BERT ë¦¬ë·° | thanks for noticing</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="keywords" content="ë…¼ë¬¸ë¦¬ë·°,NLP,NAACL,BERT,Transformers" />
    
    <meta name="description" content="BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review  Conference : NAACL Link :  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  Yea">
<meta property="og:type" content="article">
<meta property="og:title" content="ë…¼ë¬¸ BERT ë¦¬ë·°">
<meta property="og:url" content="http://eomtaehyeon.github.io/2022/09/23/Study/ML_DL/BERT%20_review/index.html">
<meta property="og:site_name" content="thanks for noticing">
<meta property="og:description" content="BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review  Conference : NAACL Link :  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  Yea">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://eomtaehyeon.github.io/images/ML_DL/BERT_Pre-t/1.png">
<meta property="article:published_time" content="2022-09-23T00:00:00.000Z">
<meta property="article:modified_time" content="2022-09-23T14:38:48.305Z">
<meta property="article:author" content="Eom Taehyeon">
<meta property="article:tag" content="ë…¼ë¬¸ë¦¬ë·°">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="NAACL">
<meta property="article:tag" content="BERT">
<meta property="article:tag" content="Transformers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://eomtaehyeon.github.io/images/ML_DL/BERT_Pre-t/1.png">
    

    

    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/titillium-web/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">


    
<script src="/libs/jquery/3.5.0/jquery.min.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                    <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/About/">About</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Settings/">Settings</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Settings/Blog/">Blog</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/">ë…¼ë¬¸ë¦¬ë·°</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/">NLP</a></li></ul></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a><ul class="main-nav-list-child"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a></li></ul></li></ul>
                                
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/">ë…¼ë¬¸ë¦¬ë·°</a><i class="icon fa fa-angle-right"></i><a class="page-title-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <article id="post-Study/ML_DL/BERT _review" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        ë…¼ë¬¸ BERT ë¦¬ë·°
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
  <div class="article-date">
    <i class="fa fa-calendar"></i>
    <a href="/2022/09/23/Study/ML_DL/BERT%20_review/" class="article-date">
       <time datetime="2022-09-23T00:00:00.000Z" itemprop="datePublished">2022-09-23</time>
    </a>
  </div>


<div class="article-date">
  <i class="fa fa-calendar-plus-o"></i>
  <a href="/2022/09/23/Study/ML_DL/BERT%20_review/" class="article-date">
     <time datetime="2022-09-23T14:38:48.305Z" itemprop="dateModified">2022-09-23</time>
  </a>
</div>


                

                
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/BERT/" rel="tag">BERT</a>, <a class="tag-link-link" href="/tags/NAACL/" rel="tag">NAACL</a>, <a class="tag-link-link" href="/tags/NLP/" rel="tag">NLP</a>, <a class="tag-link-link" href="/tags/Transformers/" rel="tag">Transformers</a>, <a class="tag-link-link" href="/tags/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/" rel="tag">ë…¼ë¬¸ë¦¬ë·°</a>
    </div>

                

                

            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            

            

            

            <p>BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review</p>
<ul>
<li>Conference : NAACL</li>
<li>Link :</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<ul>
<li>Year : 2018</li>
<li>ì €ì : Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</li>
<li>Github :</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></p>
<ul>
<li>ë™ì˜ìƒ :</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://vimeo.com/365139010">https://vimeo.com/365139010</a></p>
<h1 id="BERTë€"><a href="#BERTë€" class="headerlink" title="BERTë€?"></a>BERTë€?</h1><p>Bidirectional Encoder Representations from Transformers. </p>
<p>Unlike recent language representation models.</p>
<p>BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.</p>
<p>As a result, the pre-trained BERT model can be fine tuned with just one additional output layer to create state-of-the art models for a wide range of tasks, such as question answering and language inference, without substanial task specific architecure modifications.</p>
<h1 id="í•µì‹¬-ìš”ì•½"><a href="#í•µì‹¬-ìš”ì•½" class="headerlink" title="í•µì‹¬ ìš”ì•½"></a>í•µì‹¬ ìš”ì•½</h1><ul>
<li><p>trasformerì˜ encoder networkë¥¼ ê¸°ë°˜ìœ¼ë¡œ, self-attentionì„ ì´ìš©í•˜ì—¬ bidirectionalí•˜ê²Œ ì–¸ì–´ íŠ¹ì„±ì„ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<ul>
<li>transformerë€?</li>
</ul>
<p>  transformerëŠ” the mechanism of self-attention<strong>ì„ ì±„íƒí•˜ëŠ” ë”¥ ëŸ¬ë‹ ëª¨ë¸ë¡œì„œ, ì…ë ¥ ë°ì´í„°ì˜ ê° ë¶€ë¶„ì˜ ì¤‘ìš”ì„±ì„ ì°¨ë“±ì ìœ¼ë¡œ ê°€ì¤‘ì‹œí‚¨ë‹¤. ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ì»´í“¨í„° ë¹„ì „(CV) ë¶„ì•¼ì—ì„œ ì‚¬ìš©ëœë‹¤.</strong></p>
<ul>
<li>transformer background</li>
</ul>
<p>  before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers are built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention.</p>
<p>  transformer ì´ì „ì— ëŒ€ë¶€ë¶„ì˜ ìµœì²¨ë‹¨ NLP ì‹œìŠ¤í…œì€ LSTMê³¼ ê²Œì´íŠ¸ ë°˜ë³µ ìœ ë‹›(GRU)ê³¼ ê°™ì€ ê²Œì´íŠ¸ RNNì— ì˜ì¡´í–ˆìœ¼ë©° ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì¶”ê°€ë˜ì—ˆë‹¤. ë³€ì••ê¸°ëŠ” RNN êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì´ëŸ¬í•œ ì£¼ì˜ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì–´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œë„ RNNì˜ ì„±ëŠ¥ì„ ì£¼ì˜ì™€ ì¼ì¹˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê°•ì¡°í•œë‹¤.</p>
<ul>
<li>Encoder</li>
</ul>
<p>  Each encoder consists of two major components : a self-attention mechanism and a feed-forward neural network. The self-attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings.</p>
<p>  ê° ì¸ì½”ë”ëŠ” self-attention mechanismê³¼ feed-forward neural network ì´ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. ìì²´ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì´ì „ ì¸ì½”ë”ë¡œë¶€í„° ì…ë ¥ ì¸ì½”ë”©ì„ ë°›ì•„ë“¤ì´ê³  ì¶œë ¥ ì¸ì½”ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì„œë¡œ ê´€ë ¨ì„±ì„ í‰ê°€í•œë‹¤.</p>
<p>  ì°¸ì¡° : <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)?msclkid=a821d608bd7b11ec9cad602d05a50e33">Transformer (machine learning model) - Wikipedia</a></p>
<p>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<p>  ì°¸ì¡° : </p>
<p>  <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mxGCEWOxfe8">https://www.youtube.com/watch?v=mxGCEWOxfe8</a>
  </p>
</li>
<li><p>MLM(Masked language model)ê³¼ NSP(next sentence prediction)ë“±ì˜ pre-trainingë°©ë²•ì„ ì œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤.</p>
<ul>
<li>MLM</li>
</ul>
<p>  In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.</p>
<p>  deep bidirectional representationì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ ì…ë ¥ í† í°ì˜ ì¼ë¶€ ë¹„ìœ¨ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•œ ë‹¤ìŒ, ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•œë‹¤.</p>
<p>  ì°¸ì¡° : </p>
<p>  <a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.240.pdf?msclkid=3edd2a0dbe0211ecab2f655db1333859"></a></p>
<ul>
<li>NSP</li>
</ul>
</li>
<li><p>pre-trainingë°©ë²•ìœ¼ë¡œ feature representationì„ í•™ìŠµí•œ ë’¤ fine-tuningë§Œìœ¼ë¡œ down-stream taskë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.</p>
</li>
</ul>
<h1 id="Introduction-amp-Related-Work"><a href="#Introduction-amp-Related-Work" class="headerlink" title="Introduction &amp; Related Work"></a>Introduction &amp; Related Work</h1><p>ì €ìëŠ” ë¨¼ì € ì‚¬ì „ í›ˆë ¨ì„ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ Taskì˜ ì„±ëŠ¥ì„ í–¥ìƒ ì‹œí‚¤ëŠ” PLM(pre-trained language model)ì˜ ì‚¬ë¡€ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>Feature-based<ul>
<li>ì˜ˆì‹œ)ELMo : pre-trained representation ì„ ì¶”ê°€ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Taskì— íŠ¹í™”ëœ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>fine-tuning<ul>
<li>ì˜ˆì‹œ) GPT : taskì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì†Œí™” í•˜ê³ , ì‚¬ì „ í•™ìŠµëœ íŒŒë¼ë¯¸í„° ì „ë¶€ë¥¼ fine-tuning í•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
<p>ë‘ ë°©ì‹ ëª¨ë‘ pre-trainingì—ì„œ ë™ì¼í•œ objectiveë¥¼ ì‚¬ìš©í•˜ê³ , unidirectional language modelì„ ê°€ì •í•©ë‹ˆë‹¤. ì´ ë•Œë¬¸ì—, ë¬¸ë§¥ì„ ë‹¨ ë°©í–¥ìœ¼ë¡œë§Œ ìŠµë“í•˜ê²Œ ë˜ì–´, ì–‘ë°©í–¥ì˜ ë¬¸ë§¥ ì´í•´ê°€ í•„ìš”í•œ ë¬¸ë‹µ task ë“±ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ ëª»í•¨ì„ ì§€ì í•©ë‹ˆë‹¤.</p>
<p>ì´ì— ë…¼ë¬¸ì€, fine-tuning ë°©ì‹ì„ ê°œì„ í•˜ëŠ” PLMìœ¼ë¡œ BERT(Bidirectional Encoder Representations from Transformers) êµ¬ì¡°ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. BERTì˜ pre-trainingì€ masked language model(MLM) ê³¼ next-sentence-prediction task(NSP)ë¥¼ í†µí•´ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p><img src="/images/ML_DL/BERT_Pre-t/1.png" alt="ì¶œë ¥ ê³„ì¸µì„ ì œì™¸í•˜ê³ , ì‚¬ì „ í›ˆë ¨ê³¼ ë¯¸ì„¸ ì¡°ì • ëª¨ë‘ì—ì„œ ë™ì¼í•œ ì•„í‚¤í…ì²˜ê°€ ì‚¬ìš©ëœë‹¤. ë™ì¼í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ëŠ” ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ë¯¸ì„¸ ì¡°ì • ì¤‘ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. [CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤."></p>
<p>ì¶œë ¥ ê³„ì¸µì„ ì œì™¸í•˜ê³ , ì‚¬ì „ í›ˆë ¨ê³¼ ë¯¸ì„¸ ì¡°ì • ëª¨ë‘ì—ì„œ ë™ì¼í•œ ì•„í‚¤í…ì²˜ê°€ ì‚¬ìš©ëœë‹¤. ë™ì¼í•œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë§¤ê°œ ë³€ìˆ˜ëŠ” ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì— ëŒ€í•œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ë¯¸ì„¸ ì¡°ì • ì¤‘ì—ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ë¯¸ì„¸ ì¡°ì •ë©ë‹ˆë‹¤. [CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸&#x2F;ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤.</p>
<p>BERTëŠ” í¬ê²Œ pre-training ë‹¨ê³„ì™€ fine-tuning ë‹¨ê³„, ë‘ê°€ì§€ ë‹¨ê³„ë¡œ êµ¬ë¶„í•˜ë©°, ê° ë‹¨ê³„ëŠ” ëª¨ë‘ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.</p>
<p>Pre-training ë‹¨ê³„ì—ì„œëŠ” ë ˆì´ë¸”ë§ í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.</p>
<p>Fine-tuningì—ì„œëŠ” pre-trained íŒŒë¼ë¯¸í„°ë¡œ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ë ˆì´ë¸”ë§ëœ ë°ì´í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>BERTì˜ êµ¬ì¡°ëŠ” ë‹¤ì¸µë ˆì´ì–´ë¡œ êµ¬ì„±ëœ ì–‘ë°©í–¥ Transformerì˜ Encoderë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©°, ì•„ë˜ì™€ ê°™ì€ í‘œê¸°ë¡œ ëª¨ë¸ ì†ì„±ì„ ë‚˜íƒ€ë‚´ì—ˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>L : Layer ê°¯ìˆ˜ (ex : Transformer Block)</li>
<li>H : Hidden size</li>
<li>A : self-attention head ì˜ ê°¯ìˆ˜</li>
</ul>
<p>ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ ë‘ ëª¨ë¸ì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>BERT-BASE : L &#x3D; 12, H &#x3D; 768, A &#x3D; 12, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ &#x3D; 110M (GPT ì™€ ë™ì¼)</li>
<li>BERT-LATGE : L &#x3D; 24, H &#x3D; 1024, A &#x3D; 16, ì´ íŒŒë¼ë¯¸í„° ìˆ˜ &#x3D; 240M</li>
</ul>
<h2 id="Input-x2F-Output-Representations"><a href="#Input-x2F-Output-Representations" class="headerlink" title="Input&#x2F;Output Representations"></a>Input&#x2F;Output Representations</h2><p>ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ë‹¤ì–‘í•œ taskì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ë‹¨ì¼ ë¬¸ì¥ê³¼ ìŒìœ¼ë¡œ ì´ì–´ì§„ ë¬¸ìì„ ëª¨ë‘ í•˜ë‚˜ì˜ Sequence ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œëŠ” WordPiece embedding ì„ ì‚¬ìš©í•˜ë©° 30,000ê°œì˜ token vocalbularyë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>
<p><img src="/images/ML_DL/BERT_Pre-t/2.png"></p>
<p>Inputì€ Token embedding + Segment embedding + Position embedding ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.</p>
<ul>
<li>Token Embedding<ul>
<li>Sequence ì˜ ì²« í† í°ì€ [CLS]í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.</li>
<li>ë‘ ë¬¸ì¥ì´ ì´ì–´ì§„ ê²½ìš°, [SEP]í† í°ìœ¼ë¡œ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ë©°, ë§ˆì§€ë§‰ì—ë„ [SEP] í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.<ul>
<li>[CLS]ëŠ” ëª¨ë“  ì…ë ¥ ì˜ˆì œ ì•ì— ì¶”ê°€ëœ íŠ¹ìˆ˜ ê¸°í˜¸ì´ê³  [SEP]ëŠ” íŠ¹ìˆ˜ êµ¬ë¶„ í† í°(ì˜ˆ: ì§ˆë¬¸&#x2F;ë‹µë³€ êµ¬ë¶„)ì…ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
</li>
<li>Segment Embedding<ul>
<li>ë‘ ë¬¸ì¥ì´ ìˆì„ ë•Œ, ê°ê°ì˜ ë¬¸ì¥ì— sentence A &#x2F; sentence B ì„ë² ë”©ì„ ì ìš©í•©ë‹ˆë‹¤.</li>
</ul>
</li>
<li>Positional Embedding<ul>
<li>Transformer ì˜ ì‚¼ê°í•¨ìˆ˜ Encoding ì´ ì•„ë‹Œ, lookup table ì—ì„œ ê° positionì˜ vectorë“¤ì„ ì°¾ì•„ì„œ Position ì„ embeddingí•©ë‹ˆë‹¤.</li>
</ul>
</li>
</ul>
<h1 id="Pre-training-BERT"><a href="#Pre-training-BERT" class="headerlink" title="Pre-training BERT"></a>Pre-training BERT</h1><p>ì „í†µì ì¸ left-to-right &#x2F; right-to-left LM ì„ ì‚¬ìš©í•´ì„œ pre-train í•˜ëŠ” ELMo, GPTì™€ëŠ” ë‹¤ë¥´ê²Œ, BERTëŠ” 2ê°œì˜ unsupervised taskë¥¼ ì´ìš©í•´ì„œ ì‚¬ì „í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.</p>
<h2 id="Task-1-Masked-LM"><a href="#Task-1-Masked-LM" class="headerlink" title="Task #1 : Masked LM"></a>Task #1 : Masked LM</h2><p>ê¸°ì¡´ì˜ ì–¸ì–´ ëª¨ë¸ì„ Bidirectional í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²½ìš°, ê°„ì ‘ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë ¤ëŠ” ë‹¨ì–´ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ ë˜ì–´ ì˜ˆì¸¡ ìì²´ê°€ ë¬´ì˜ë¯¸í•´ì§ˆ ìˆ˜ ìˆìŒì„ ì§€ì í•©ë‹ˆë‹¤.</p>
<p>BERTëŠ” ì „ì²´ Sequence ì—ì„œ 15%ì˜ í† í°ì„ ê°€ë¦¬ëŠ” Mask ë¥¼ ì¶”ê°€í•˜ì—¬ ì–‘ë°©í–¥ í•™ìŠµì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.</p>
<p>ë˜í•œ, ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ì´ [MASK] í† í°ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” Fine-tuningì—ë„ ì ìš©ë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Masking Ruleì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>80%ëŠ” [MASK]ë¡œ ì¹˜í™˜</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my dog is hairy -&gt; my dog is [MASK]</span><br></pre></td></tr></table></figure>

<ul>
<li>10%ëŠ” ëœë¤í•œ í† í°ìœ¼ë¡œ ì¹˜í™˜</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my dog is hairy -&gt; my dog is apple</span><br></pre></td></tr></table></figure>

<ul>
<li>10%ëŠ” ê¸°ì¡´ì˜ í† í°ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my dog is hairy -&gt; my dog is hairy</span><br></pre></td></tr></table></figure>

<p>ì´ë¡œ ì¸í•´ modelì€ ë‹¨ì–´ì˜ ê¸°ì›(ì›ë³¸&#x2F;MASK&#x2F;random changed)ì„ ì•Œì§€ ëª»í•œ ì±„ ëª¨ë“  input tokenì— ëŒ€í•´ì„œ distributional contextual representationì„ ìœ ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ë˜í•œ ì „ì²´ì˜ 1.5% í† í°ë§Œì´ ëœë¤í•˜ê²Œ ë³€ê²½ë˜ì—ˆê¸°ì—, ëª¨ë¸ì´ ì˜ëª» í•™ìŠµë  ìš°ë ¤ ë˜í•œ ì ìŒì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<p><img src="/images/ML_DL/BERT_Pre-t/3.png"></p>
<p>ì´ëŸ¬í•œ ë¹„ìœ¨ì€ ìœ„ì˜ ì‹¤í—˜ì„ í†µí•´ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>ì „ì²´ì ìœ¼ë¡œ fine-tuning task ê°€ feature-based ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
<li>íŠ¹ì • Masking Rule ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, Rule í˜¼í•©ì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
</ul>
<p>ìµœì¢…ì ìœ¼ë¡œëŠ” cross-entropy loss ë¥¼ ì‚¬ìš©í•´ì„œ ê¸°ì¡´ì˜ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<h2 id="Task-2-Next-Sentence-Prediction-NSP"><a href="#Task-2-Next-Sentence-Prediction-NSP" class="headerlink" title="Task #2: Next Sentence Prediction (NSP)"></a>Task #2: Next Sentence Prediction (NSP)</h2><ul>
<li><p>Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding  he relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the entences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP).</p>
  <aside>
  ğŸ’¡ ë¬¸ì¥ ê´€ê³„ë¥¼ ì´í•´í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ëª¨ë“  ë‹¨ì¼ ì–¸ì–´ ë§ë­‰ì¹˜ì—ì„œ ì‚¬ì†Œí•œ ê²ƒìœ¼ë¡œ ìƒì„±ë  ìˆ˜ ìˆëŠ” ì´í•­í™”ëœ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡ ì‘ì—…ì„ ìœ„í•´ ì‚¬ì „ í›ˆë ¨í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, ê° ì‚¬ì „ í›ˆë ¨ ì˜ˆì— ëŒ€í•œ ë¬¸ì¥ Aì™€ Bë¥¼ ì„ íƒí•  ë•Œ, ì‹œê°„ì˜ 50%ëŠ” Aë¥¼ ë”°ë¥´ëŠ” ì‹¤ì œ ë‹¤ìŒ ë¬¸ì¥(IsNextë¡œ ë ˆì´ë¸”ë§ë¨)ì´ê³ , 50%ëŠ” ë§ë­‰ì¹˜(NotNextë¡œ ë ˆì´ë¸”ë§ë¨)ì˜ ë¬´ì‘ìœ„ ë¬¸ì¥ì´ë‹¤. ê·¸ë¦¼ 1ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, CëŠ” ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(NSP)ì— ì‚¬ìš©ëœë‹¤.
  
  </aside></li>
</ul>
<p>Question-answering(QA), Natural Language Interference(NLI) ë“±ì˜ taskëŠ” ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´í•´í•´ì•¼ í•˜ëŠ” taskì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ LM ìœ¼ë¡œëŠ” í•´ë‹¹ Task ì˜ í•™ìŠµì´ ì–´ë ¤ìš°ë¯€ë¡œ, ì´ ë˜í•œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.</p>
<p>ë‘ ë¬¸ì¥ê³¼ ë ˆì´ë¸”ë¡œ êµ¬ì„±ëœ ë‹¤ìŒì˜ ë°ì´í„° ì…‹ìœ¼ë¡œ Binary ë¶„ë¥˜ ë¬¸ì œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<ul>
<li>50% : ì‹¤ì œë¡œ ì´ì–´ì§„ ë‘ ë¬¸ì¥ì„ ì œì‹œ : ë ˆì´ë¸” IsNext</li>
<li>50% : ê´€ê³„ê°€ ì—†ëŠ” ì„ì˜ì˜ ë¬¸ì¥ì„ ì œì‹œ : ë ˆì´ë¸” NotNext</li>
</ul>
<h3 id="Pre-training-data"><a href="#Pre-training-data" class="headerlink" title="Pre-training data"></a>Pre-training data</h3><p>ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•´ ì‚¬ìš©í•œ corpus ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>BooksCorpus (800M words)</li>
<li>English Wikipedia (2,500M words) :  text passage ë§Œ ì‚¬ìš©í–ˆê³ , ëª©ë¡ì´ë‚˜ í‘œ ë“±ì€ ì œì™¸í•˜ì—¬ ì‚¬ìš©</li>
</ul>
<p>ê¸´ ë¬¸ë§¥ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ Billion Word Benchmark ì™€ ê°™ì´ ì„ì¸ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.</p>
<h2 id="Fine-tuning-BERT"><a href="#Fine-tuning-BERT" class="headerlink" title="Fine-tuning BERT"></a>Fine-tuning BERT</h2><p>Task ë³„ ì…ë ¥ì˜ ê°œìˆ˜(ë‹¨ì¼ ë¬¸ì¥, 2ê°œì˜ ë¬¸ì¥)ì— ë”°ë¼ í•˜ë‚˜ì˜ sequence ë¥¼ ìƒì„±í•˜ì—¬ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.</p>
<p>ì´í›„, íŒŒë¼ë¯¸í„°ë“¤ì„ í•´ë‹¹ taskì— ë§ê²Œ end-to-endë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.</p>
<p>Sequence tagging ì´ë‚˜ question answering ê°™ì´ token-level task ë“¤ì˜ ê²½ìš°, ë§ˆì§€ë§‰ transformer layerì˜ token ë“¤ì„ ì‚¬ìš©í•˜ì—¬ fine-tuning í•©ë‹ˆë‹¤.</p>
<p>Sentence Classification, sentiment analysis ë“±ì˜ sentence-level classification task ë“¤ì€ ë§ˆì§€ë§‰ layerì˜ CLS tokenì˜ hidden stateë¥¼ fine-tuningì— ì´ìš©í•©ë‹ˆë‹¤.</p>
<p>Pre-trainingê³¼ ë¹„êµí–ˆì„ ë•Œ, fine-tuning ì€ ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>BERT fine-tuningì„ ì´ìš©í•œ 11ê°œì˜ NLP taskì˜ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  Task ì—ì„œ State-Of-Arts ë¥¼ ë‹¬ì„±í•˜ì˜€ê³ , ê°ê°ì˜ íŠ¹ì„±ì— ë§ëŠ” í•™ìŠµ ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<p><img src="/images/ML_DL/BERT_Pre-t/4.png"></p>
<h2 id="GLUE"><a href="#GLUE" class="headerlink" title="GLUE"></a>GLUE</h2><p>GLEU benchmarkëŠ” ë‹¤ì–‘í•œ natural language understanding taskë¥¼ ìœ„í•œ ë¬¸ì¥ ë¶„ë¥˜ Task ì…ë‹ˆë‹¤. BERT ëª¨ë¸ì— ë¶„ë¥˜ë¥¼ ìœ„í•œ classification layerë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.</p>
<h2 id="SQuAD-v1-1"><a href="#SQuAD-v1-1" class="headerlink" title="SQuAD v1.1"></a>SQuAD v1.1</h2><p>SQuAD v1.1 datasetì€ Question Answering datasetìœ¼ë¡œ, ì§ˆë¬¸ê³¼ ì§€ë¬¸ì˜ í˜•íƒœ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë‹µë³€ì„ ì°¾ëŠ” ê³¼ì œì…ë‹ˆë‹¤. BERT ëŠ” ì§ˆë¬¸ê³¼ ì§€ë¬¸ì„ í•˜ë‚˜ì˜ single sequence ë¡œ ë¬¶ì–´ì„œ inputìœ¼ë¡œ ë§Œë“  ë’¤, ì§€ë¬¸ì—ì„œ ì •ë‹µì´ ë  ìˆ˜ ìˆëŠ” ì˜ì—­ì„ ì°¾ëŠ” ë°©ì‹ìœ¼ë¡œ Task ë¥¼ ì „í™˜í•´ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<h2 id="SQuAD-v2-0"><a href="#SQuAD-v2-0" class="headerlink" title="SQuAD v2.0"></a>SQuAD v2.0</h2><p>SQuAD v2.0 ì€ 1.1 ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ì§€ë¬¸ë§Œìœ¼ë¡œëŠ” ëŒ€ë‹µì´ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸ì´ í¬í•¨ëœ dataset ì…ë‹ˆë‹¤. BERT ëŠ” ëŒ€ë‹µì´ ë¶ˆê°€ëŠ¥í•œì§€ ì—¬ë¶€ë¥¼ CLS token ì„ ì´ìš©í•´ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.</p>
<h2 id="SWAG"><a href="#SWAG" class="headerlink" title="SWAG"></a>SWAG</h2><p>The Situations With Adversarial Generations (SWAG) datasetì€ ì• ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë³´ê¸°ë¡œ ì£¼ì–´ì§„ 4 ë¬¸ì¥ ì¤‘ ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ë¬¸ì¥ì„ ì°¾ëŠ” task ì…ë‹ˆë‹¤.</p>
<p>Fine-tuning ì„ ìœ„í•´, ì• ë’¤ ë¬¸ì¥ì„ ì¡°í•©í•´ 4ê°œì˜ ë¬¸ì¥ì„ ìƒì„±í•˜ê³ , í•´ë‹¹ sequence ì˜ ì •ë‹µ ì—¬ë¶€ë¥¼ Classification í•˜ëŠ” Task ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. </p>
<h1 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h1><h2 id="Effect-of-Pre-training-Tasks"><a href="#Effect-of-Pre-training-Tasks" class="headerlink" title="Effect of Pre-training Tasks"></a>Effect of Pre-training Tasks</h2><p><img src="/images/ML_DL/BERT_Pre-t/5.png"></p>
<p>ì‚¬ì „ í›ˆë ¨ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´, í›ˆë ¨ì„ ì œê±°í•˜ë©° ì‹¤í—˜í•œ ê²°ê³¼ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>No NSP : masked LM(MLM) ìœ¼ë¡œë§Œ í•™ìŠµë˜ê³  NSPëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°<ul>
<li>NLI ë¬¸ì œì—ì„œ ì„±ëŠ¥ì´ í•˜ë½</li>
<li>NSP ê°€ ë¬¸ì¥ê°„ì˜ ë…¼ë¦¬ì ì¸ êµ¬ì¡° íŒŒì•…ì— ì¤‘ìš”í•œ ì—­í• ì„ ìˆ˜í–‰í•¨ì„ ì‹œì‚¬</li>
</ul>
</li>
<li>LTR &amp; No NSP : MLMì´ ì•„ë‹Œ Left-To-Right model ì„ ì‚¬ìš©í•˜ê³  NSPë„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°<ul>
<li>ëª¨ë“  taskì— ëŒ€í•´ì„œ ì„±ëŠ¥ì´ ê°ì†Œí•˜ë©°, íŠ¹íˆ MRPCì™€ SQuAD ì—ì„œ í° í­ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ í™•ì¸</li>
</ul>
</li>
<li>LTR &amp; No NSP + BiLSTM : BiLSTM ì„ ì¶”ê°€í•˜ì—¬ ì–‘ë°©í–¥ì„±ì„ ì œê³µ<ul>
<li>BiLSTM ì´ ì—†ëŠ” ê²½ìš°ì— ë¹„í•´ ë¯¸ë¯¸í•˜ê²Œ ì„±ëŠ¥ì´ ìƒìŠ¹í•˜ì§€ë§Œ, MLM ì— ë¹„í•´ì„œëŠ” ë–¨ì–´ì§</li>
<li>MLMì´ bi-directionality ê°€ ë” ê°•í•¨ì„ ì‹œì‚¬</li>
</ul>
</li>
</ul>
<h2 id="Effect-of-Model-size"><a href="#Effect-of-Model-size" class="headerlink" title="Effect of Model size"></a>Effect of Model size</h2><p><img src="/images/ML_DL/BERT_Pre-t/7.png"></p>
<p>ëª¨ë¸ì˜ í¬ê¸°ê°€ fine-tuning ì •í™•ë„ì— ì–´ë– í•œ ì˜í–¥ì„ ì£¼ëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.</p>
<ul>
<li>ëª¨ë¸ì´ ì»¤ì§ˆìˆ˜ë¡, Pre-training ì˜ ì •í™•ë„ê°€ ìƒìŠ¹</li>
<li>downstream task ê°€ ì‘ì€ ìŠ¤ì¼€ì¼ì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ë•Œì—ë„ ì„±ëŠ¥ì´ ìƒìŠ¹</li>
</ul>
<h2 id="Feature-based-Approach-with-BERT"><a href="#Feature-based-Approach-with-BERT" class="headerlink" title="Feature-based Approach with BERT"></a>Feature-based Approach with BERT</h2><p>BERTë¥¼ ELMo ì™€ ê°™ì€ feature-based ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ê³¼ ê·¸ ì¥ì ì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<ul>
<li>task-specific model ì˜ ì¶”ê°€ë¡œ Transformer encoder ë§Œìœ¼ë¡œ ìˆ˜í–‰ ë¶ˆê°€ëŠ¥í•œ taskì— ì ìš© ê°€ëŠ¥</li>
<li>Update parameter ê°ì†Œë¡œ í•™ìŠµ ë¹„ìš© ì ˆê°</li>
</ul>
<p><img src="/images/ML_DL/BERT_Pre-t/8.png"></p>
<p>CoNLL-2003 Named Entity Recognition task ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.</p>
<p>BERT ë ˆì´ì–´ì˜ ì¼ë¶€ activation ì— Bi-LSTMì„ ë¶€ì°©ì‹œì¼œ í•´ë‹¹ ë ˆì´ì–´ë§Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ Feature-based approach ë¥¼ êµ¬í˜„í•˜ì˜€ê³ , ê¸°ì¡´ ELMo ì— ë¹„í•´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.</p>
<p>ë˜í•œ Fine-Tuning ë°©ë²•ë§Œì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œë„, SOTA ì— ê·¼ì ‘í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŒì„ ì œì‹œí•©ë‹ˆë‹¤.</p>
<p><img src="/images/ML_DL/BERT_Pre-t/9.png"></p>
<p>ì°¸ì¡° : </p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=30SvdoA6ApE">https://www.youtube.com/watch?v=30SvdoA6ApE</a></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>RNNì€ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤ë©´ TransformerëŠ” í•œë²ˆì— ê³„ì‚°.</li>
<li>BERTëŠ” Transformerë¥¼ ì´ìš©í•´ì„œ ì–‘ë°©í–¥ì˜ ë¬¸ë§¥ì„ ìˆ«ìì˜ í˜•íƒœë¡œ ë°”ê¿”ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ë‹¤.</li>
</ul>

        </div>
        <footer class="article-footer">
            



    <a data-url="http://eomtaehyeon.github.io/2022/09/23/Study/ML_DL/BERT%20_review/" data-id="cl8el1faw0000lgto60wpfgyb" class="article-share-link"><i class="fa fa-share"></i>Share</a>
<script>
    (function ($) {
        $('body').on('click', function() {
            $('.article-share-box.on').removeClass('on');
        }).on('click', '.article-share-link', function(e) {
            e.stopPropagation();

            var $this = $(this),
                url = $this.attr('data-url'),
                encodedUrl = encodeURIComponent(url),
                id = 'article-share-box-' + $this.attr('data-id'),
                offset = $this.offset(),
                box;

            if ($('#' + id).length) {
                box = $('#' + id);

                if (box.hasClass('on')){
                    box.removeClass('on');
                    return;
                }
            } else {
                var html = [
                    '<div id="' + id + '" class="article-share-box">',
                        '<input class="article-share-input" value="' + url + '">',
                        '<div class="article-share-links">',
                            '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
                            '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
                            '<a href="http://pinterest.com/pin/create/button/?url=' + encodedUrl + '" class="article-share-pinterest" target="_blank" title="Pinterest"></a>',
                            '<a href="https://plus.google.com/share?url=' + encodedUrl + '" class="article-share-google" target="_blank" title="Google+"></a>',
                        '</div>',
                    '</div>'
                ].join('');

              box = $(html);

              $('body').append(box);
            }

            $('.article-share-box.on').hide();

            box.css({
                top: offset.top + 25,
                left: offset.left
            }).addClass('on');

        }).on('click', '.article-share-box', function (e) {
            e.stopPropagation();
        }).on('click', '.article-share-box-input', function () {
            $(this).select();
        }).on('click', '.article-share-box-link', function (e) {
            e.preventDefault();
            e.stopPropagation();

            window.open(this.href, 'article-share-box-window-' + Date.now(), 'width=500,height=450');
        });
    })(jQuery);
</script>

        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "Eom Taehyeon"
        },
        "headline": "ë…¼ë¬¸ BERT ë¦¬ë·°",
        "image": "http://eomtaehyeon.github.io/images/ML_DL/BERT_Pre-t/1.png",
        "keywords": "ë…¼ë¬¸ë¦¬ë·° NLP NAACL BERT Transformers",
        "genre": "ë…¼ë¬¸ë¦¬ë·° NLP",
        "datePublished": "2022-09-23",
        "dateCreated": "2022-09-23",
        "dateModified": "2022-09-23",
        "url": "http://eomtaehyeon.github.io/2022/09/23/Study/ML_DL/BERT _review/",
        "description": "BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review

Conference : NAACL
Link :

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Yea",
        "wordCount": 1749
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>


    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="user" href="https://github.com/eomtaehyeon/Resume" target="_blank" rel="noopener">
                        <i class="icon fa fa-user"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="instagram" href="https://www.instagram.com/hi__gorae/" target="_blank" rel="noopener">
                        <i class="icon fa fa-instagram"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/eomtaehyeon" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2022/09/22/Study/DataQ/BigData/Class1.2_Data_Analysis_Plan/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">1ê³¼ëª©. ë¹…ë°ì´í„° ë¶„ì„ ê¸°íš 2ì¥. ë°ì´í„° ë¶„ì„ ê³„íš</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/">ë…¼ë¬¸ë¦¬ë·°</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2022/09/23/Study/ML_DL/BERT%20_review/" class="title">ë…¼ë¬¸ BERT ë¦¬ë·°</a></p>
                            <p class="item-date"><time datetime="2022-09-23T00:00:00.000Z" itemprop="datePublished">2022-09-23</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a></p>
                            <p class="item-title"><a href="/2022/09/22/Study/DataQ/BigData/Class1.2_Data_Analysis_Plan/" class="title">1ê³¼ëª©. ë¹…ë°ì´í„° ë¶„ì„ ê¸°íš 2ì¥. ë°ì´í„° ë¶„ì„ ê³„íš</a></p>
                            <p class="item-date"><time datetime="2022-09-22T00:00:00.000Z" itemprop="datePublished">2022-09-22</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a></p>
                            <p class="item-title"><a href="/2022/09/21/Study/DataQ/BigData/Class1.1_Understanding_Big_Data/" class="title">1ê³¼ëª©. ë¹…ë°ì´í„° ë¶„ì„ ê¸°íš 1ì¥. ë¹…ë°ì´í„°ì˜ ì´í•´</a></p>
                            <p class="item-date"><time datetime="2022-09-21T00:00:00.000Z" itemprop="datePublished">2022-09-21</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/About/">About</a></p>
                            <p class="item-title"><a href="/2022/09/20/About/eomtaehyeon/" class="title">I am ~</a></p>
                            <p class="item-date"><time datetime="2022-09-20T10:25:18.896Z" itemprop="datePublished">2022-09-20</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Settings/">Settings</a><i class="icon fa fa-angle-right"></i><a class="article-category-link" href="/categories/Settings/Blog/">Blog</a></p>
                            <p class="item-title"><a href="/2022/09/16/Github/Hexo/HexoThemeSetting/" class="title">Hexoë¸”ë¡œê·¸ í…Œë§ˆì ìš©í•˜ê¸°</a></p>
                            <p class="item-date"><time datetime="2022-09-16T00:00:00.000Z" itemprop="datePublished">2022-09-16</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/About/">About</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Settings/">Settings</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Settings/Blog/">Blog</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/">ë…¼ë¬¸ë¦¬ë·°</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/">NLP</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a><span class="category-list-count">2</span></li></ul></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a><span class="archive-list-count">6</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/" rel="tag">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/" rel="tag">Blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github/" rel="tag">Github</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NAACL/" rel="tag">NAACL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformers/" rel="tag">Transformers</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/" rel="tag">ë…¼ë¬¸ë¦¬ë·°</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/" rel="tag">ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/" rel="tag">ë¹…ë°ì´í„°</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/" rel="tag">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/" rel="tag">ë¹…ë¶„ê¸°</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/" rel="tag">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%ED%95%84%EA%B8%B0/" rel="tag">í•„ê¸°</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/BERT/" style="font-size: 10px;">BERT</a> <a href="/tags/Blog/" style="font-size: 20px;">Blog</a> <a href="/tags/Github/" style="font-size: 20px;">Github</a> <a href="/tags/Hexo/" style="font-size: 20px;">Hexo</a> <a href="/tags/NAACL/" style="font-size: 10px;">NAACL</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Transformers/" style="font-size: 10px;">Transformers</a> <a href="/tags/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/" style="font-size: 10px;">ë…¼ë¬¸ë¦¬ë·°</a> <a href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/" style="font-size: 20px;">ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸</a> <a href="/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/" style="font-size: 20px;">ë¹…ë°ì´í„°</a> <a href="/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/" style="font-size: 20px;">ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬í•„ê¸°</a> <a href="/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/" style="font-size: 20px;">ë¹…ë¶„ê¸°</a> <a href="/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/" style="font-size: 20px;">ìê²©ì¦ê³µë¶€ìš”ì ì •ë¦¬</a> <a href="/tags/%ED%95%84%EA%B8%B0/" style="font-size: 20px;">í•„ê¸°</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a target="_blank" rel="noopener" href="https://github.com/eomtaehyeon">Github</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2022 Eom Taehyeon</p>
                
                <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>. Theme by <a href="https://github.com/ppoffice" target="_blank">PPOffice</a></p>
                
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

    </div>
    
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'http://eomtaehyeon.github.io/2022/09/23/Study/ML_DL/BERT%20_review/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>





    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    

    
    
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


</body>
</html>
