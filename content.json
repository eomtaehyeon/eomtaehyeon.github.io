{"meta":{"title":"thanks for noticing","subtitle":"","description":"","author":"Eom Taehyeon","url":"http://eomtaehyeon.github.io","root":"/"},"pages":[],"posts":[{"title":"2과목. 빅데이터 탐색 3장. 통계 기법 이해","slug":"Study/DataQ/BigData/Class2.3_Understanding_Statistical_Techniques","date":"2022-09-27T00:00:00.000Z","updated":"2022-09-27T12:42:26.984Z","comments":true,"path":"2022/09/27/Study/DataQ/BigData/Class2.3_Understanding_Statistical_Techniques/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/27/Study/DataQ/BigData/Class2.3_Understanding_Statistical_Techniques/","excerpt":"","text":"3장 통계 기법 이해 빅데이터 분석의 기초가 되는 통계기법을 학습 핵심키워드 표본추출 확률분포 추정 가설검정 01. 기술통계 데이터를 요약하고 객관화 기술 통계에 대해 이해 표본의 추출과 분포에 대한 개념 1. 데이터 요약기술 통계 개념 통계학 불확실하고 잘 알려지지 않은 사실과 대상에 관련된 자료를 수집 및 요약정리하고, 이를 바탕으로 해석 및 분석하는 데 필요한 이론과 방법을 과학적으로 제시하는 학문 기술 통계학 수집된 자료를 정리하여 그림이나 표로 요약하거나 자료의 수치를 요약한 대푯값(통계량 : 평균, 분산, 상관계수 등)과 데이터 분포의 형태와 변동의 크기를 구하는 방법을 다루는 것 기술 통계와 추론 통계 기술 통계 표본 추출 표본 특성 파악 추론 통계 표본 추출 표본 특성 파악 후 일반화 여부 파악 전체 모집단의 특성을 ‘추정’ 기술 통계로 데이터 요약하기 자료(현상)의 요약 자료를 대표하는 수 대푯값이나 중심값(평균, 최빈값, 중앙값 등) 예) 명목척도인 경우 최빈값을 대푯값으로 사용 서열척도의 경우 중앙값을 대푯값으로 사용 등간척도의 경우 평균값을 대푯값으로 사용 자료(현상)의 변화 정도 파악 범위 최대 관측치와 최소 관측치의 차이 편차 개별 관측치에서 평균을 차감한 수를 편차라고 한다. 분산 평균으로부터 관측치들이 평균적으로 얼마나 떨어져 잇는지를 요약해주는 값 편차 제곱의 합을 관측치 수로 나누어서 구함 기술통계뿐만 아니라 추리통계에서도 중요한 역할 장점 변화 방향 무관, 변화의 폭을 쉽게 파악 단점 편차의 제곱이기 때문에 실제 측정치보다 매우 큰 숫자로 표현 실제 관측치의 단위 기준으로는 어느 정도 변화폭인지 파악하기 어려움 표준편차 분산에 제곱근을 적용해 구한 값 분산처럼 변화의 폭을 쉽게 파악할 수 있다. 실제 관측치의 단위와 동일한 단위로 변화를 파악할 수 있다. 2. 표본 추출표본 개념 전수조사 관측하고자 하는 데이터의 모든 범위를 조사하는 방법 표본조사 모집단의 일부분만 선택해 조사 · 분석하여 전체 집단의 특성을 추정하는 통계 조사 방법 표본 관측치들은 모집단 관측치의 대표성을 지녀야 함 모수 관심을 갖고 있는 모집단 관측 대표적인 모수 모비율, 모평균, 모총계 등 통계량 표본을 조사하여 얻은 데이터를 가지고 모수를 추정하기 위해 만든 공식을 의미 표본 추출 변동 표본을 뽑을 때마다 통계량이 달라지는 것 용어 해설 표본(sample) 큰 데이터 집합에서 얻은 부분 데이터 집합 모집단(population) 어떤 데이터 집합을 구성하는 전체 대상 또는 전체 집합 임의추출(random sampling) 무작위로 표본을 추출하는 것 무작위로 추출하기 때문에 각 추출에서 모든 데이터는 동일한 확률로 뽑힌다. 단순 임의 표본(simple random sample) 이 결과로 얻은 표본 모집단을 구간으로 나누지 않고 임의추출로 얻은 표본 복원추출(sampleing with replacement) 표본 추출 후 중복 추출이 가능하게 해당 표본을 다시 모집단에 포함해 추출하는 것 비복원추출(sampling without replacement) 표본 추출 후, 중복 추출이 안 되게 해당 표본을 다음번 추출에 사용하지 않는 것 층별 임의추출(stratified random sampling) 모집단을 구간으로 나누어 각 구간에서 무작위로 표본을 추출하는 것 단순 임의추출(simple random sampling) 모집단을 구간으로 나누지 않고 임의추출로 얻은 표본 표본편향(sample bias) 모집단을 잘못 대표하는 표본 표본 조사의 절차 표본조사의 목표 설정 모집단 정의 표본 크기 결정 표본 추출 방법 선정 조사 데이터 분석 및 결과 도출 표본 크기의 결정 통계적으로 신뢰할 수 있는 정도의 표본 크기를 결정해야 한다. 확률 표본 추출 모집단 내의 모든 대상이 표본으로 선정될 확률을 동일하게 갖게 한 후 무작위로 표본을 추출하는 방법 객관성 확보 비확률 표본 추출 모집단의 구성요소인 각 추출 단위를 뽑을 때 비확률적 방법으로 표본을 추출하는 방법 추정의 정확성을 평가할 수 없어 일반화에 어려움 3. 확률 분포확률 개념 확률 어떤 일이 일어날 가능성의 측도 확률의 종류 이론적 확률 수학적 이론을 기반으로 계산되는 확률을 의미 객관적 확률 동일 조건으로 몇 번 반복했을 때 발생할 확률을 의미 주관적 확률 관찰자의 주관적 견해로 표현되는 확률을 의미 확률분포의 개념 확률분포 확률 변수가 특정한 값을 가질 확률을 나타내는 함수 확률 변수(Random Variable) 결과를 예측할 수 없는 확률 실험에서 나타날 수 있는 확률적 결과를 수치로 표현한 값 이산확률 변수 유한하게 셀 수 있는 확률 변수 연속확률 변수 특정 범위 안에 모든 실숫값을 포함하는 경우의 확률 변수 확률분포(Probability Distribution) 확률분포함수(Probability Distribution Function) 확률 변수를 일직선상 공간에 표현한 함수 확률 질량 함수 누적 분포 함수 확률 밀도 함수 확률분포 : 확률변수의 분포 형태를 그래프로 표현한 것 확률분포함수의 3가지 특징 비감소성 : 확률변수의 값의 크기가 분포함수의 값의 크기와 같다. a &lt; b 이면, F(a) &lt; F(b) 극한성 : 최소극한값은 ‘0’, 최대극한값은 ‘1’ 우방 연속성 : 함수 그래프상 오른쪽(양의 값)으로 연속성을 갖는다. 확률분포 종류 확률 변수의 종류에 따라 나눔 이산 확률 분포(discrete probaility distribution) 이산확률변수 X가 가지는 확률 분포 확률변수 X는 하나씩 셀 수 있는 값 기댓값 확률변수 X의 가능한 모든 값의 가중 평균을 의미 이산 확률 분포의 종류 이항분포(Binomial Distribution) 포아송 분포(Poisson Distribution) 초기하 분포(Hypergeometric Distribution) 비복원추출에서 N개 중에 n번 추출했을 때 원하는 것 k개가 뽑힐 확률의 분포 기하 분포(Geometric Distribution) 다항 분포(Multinomial Distribution) 여러 개의 값을 가질 수 있는 독립 확률변수들에 대한 확률분포 여러 번의 독립적 시행에서 각각의 값이 특정 횟수가 나타날 확률을 정의 연속 확률 분포(continuous probability distribution) 확률 밀도 함수를 이용해 분포를 표현할 수 있는 경우를 의미 연속 확률 변수 연속 확률 분포를 가지는 확률 변수 연속확률 분포의 종류 균등분포 정규분포(Normal Distribution) 분포 곡선이 평균값을 중앙으로 하여 좌우 대칭으로 종 모양을 이루는 분포 표준정규분포 감마분포 특정 수(n)의 사건이 일어날 때까지 걸리는 시간에 관한 연속 확률분포 지수분포 : 감마분포의 n에 1을 대입한 경우 베타분포 2개의 변수를 갖는 특수 함수인 베타함수를 이용한 분포 지수분포 t 분포 정규분포의 한계를 보완한 분포 f 분포 두 데이터셋의 분산을 다루는 분포로서 분산의 비율을 통해 그 크기를 비교 분산에 대한 검정이나 추정을 하는 경우 많이 활용 카이제곱분포 정규분포를 제곱하거나 제곱한 것을 더한 것 이산확률분포 베르누이 시행 : 연속된 n번의 독립적 시행에서 각 시행이 확률 p를 가질 때의 이산확률 분포 연속확률분포 4. 표본분포(Sampling Distribution)표본분포 중심 극한정리의 특징 표본의 수가 작아도 모집단의 통계량을 추정할 수 있다. 모든 데이터를 정규분포로 만들 수 있다. 02. 추론 통계 모집단의 특성을 추론하기 위한 추정 가설검정을 이해하고 결과를 해석하는 방법 추론 통계(Inferential Statistics) 모집단에 대한 특성을 추론하는 과정 추정(estimation) 가설검정(test of hypotheses) 1. 점 추정점추정(point estimation) 모수를 특정한 수치로 표현하는 것 추정량의 조건 종류 불편성(unbiasedness) 효율성(efficiency) 일치성(consistency) 좋은 추정량의 조건 평균 오차제곱이 최솟값 불편성 : 추정량이 모수와 동일 일치성 : 표본의 크기가 모집단 규모에 근접 효율성(유효성) : 추정량의 분산이 최솟값 충분성 : 표본이 모집단의 대표성을 가짐 2. 구간추정구간추정(interval estimation) 모수를 최솟값과 최댓값의 범위로 추정하는 것 신뢰구간과 z 값 구간추정량 계산 방법 모비율의 신뢰구간 추정 구간추정 성질 3. 가설검정가설검정 용어 단일 검정과 양측 검정 가설검정과 오류 제1종 오류와 제2종 오류 단일 모평균 검정 단일 모비율 검정 t 검정 출처 내용 출처 이지패스 2021 빅데이터분석기사 필기(수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음 https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 2. 빅데이터 분석 기획 (3) 기술통계] - 세우초밥 https://blog.naver.com/ericalee97/222162993681 위키피디아&#x2F;확률분포 https://ko.wikipedia.org/wiki/확률_분포 [통계기초] 통계분석 : 통계적 추정 : 점추정, 구간추정 - 냉철한욱 https://warm-uk.tistory.com/24","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"2과목. 빅데이터 탐색 2장. 데이터 탐색","slug":"Study/DataQ/BigData/Class2.2_Data_Exploration","date":"2022-09-26T00:00:00.000Z","updated":"2022-09-26T08:42:37.701Z","comments":true,"path":"2022/09/26/Study/DataQ/BigData/Class2.2_Data_Exploration/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/26/Study/DataQ/BigData/Class2.2_Data_Exploration/","excerpt":"","text":"2장 데이터 탐색데이터 간의 유의미한 관계 파악 및 이해를 위한 탐색 방법들을 학습한다. 핵심 키워드 EDA 상관분석 기초통계량 다변량분석 01. 데이터 탐색 기초학습목표 데이터 탐색을 위한 기본적인 통계 확인과 관계 및 분포를 해석하는 방법을 학습 1. 데이터 탐색 개요EDA EDA의 4R 데이터 탐색 개요 개별 변수 탐색 방법 다차원 데이터 탐색 방법 JOIN 종류 2. 상관 관계 분석상관분석과 상관계수 스피어만 상관계수 켄달 상관계수 피어슨 상관계수(가장 일반적) -1과 1 사이의 값을 가지고 강도와 방향의 측면에서 해석할 수 있다. 상관계수 해석 강도 : 상관계수의 절댓값이 클수록 강한 상관이 있다. 상관계수 ±1의 극단 값 : 실제로 거의 존재하지 않으며 완전한 선형관계를 의미 상관계수 0 : 선형의 상관관계가 없음을 의미 방향 : 상관계수의 부호는 관계의 방향을 의미 양(+)의 상관계수 : 한 변수의 값이 증가함에 따라 다른 변수의 값도 증가 음(-)의 상관계수 : 한 변수의 값이 증가함에 따라 다른 변수의 값은 감소 상관계수에 따른 그래프 유형 피어슨 상관 계수 vs. 스피어만 상관계수 3. 기초통계량 추출 및 이해기초통계량 추출 및 이해 중심경향치 : 단일 값으로 전체 데이터를 대표할 수 있게 중앙에 위치한 데이터를 표현 산포도 : 데이터의 흩어진 정도를 설명하는 통계치 왜도 : 데이터 분포의 비대칭성을 나타내는 지표 첨도 : 데이터들이 분포의 중심에 어느 정도 몰려 있는 가를 측정할 때 사용하는 지표 4. 시각적 데이터 탐색시각적 데이터 탐색 - 그래프 종류 히스토그램 막대그래프 줄기-잎 그림 상자그림 산점도 원그래프 02. 고급 데이터 탐색학습목표 현실 속의 다양한 형태로 존재하는 데이터를 처리하고 분석하는 방법 1. 시공간 데이터 탐색시간 데이터의 이해 날짜와 시간 데이터는 실생활에서 자주 접할 수 있는 데이터 형태로 유용한 정보를 제공 R에서 자주 사용하는 시간 포맷 형식 공간분석과 GIS GIS 구성요소 시공간 데이터 공간적 객체에 시간의 개념이 추가되어 시간에 따라 위치나 형상이 변하는 데이터 시공간 데이터의 특징 시공간 데이터의 타입 포인트 타입 : 하나의 노드로 구성되는 공간 데이터 타입 라인 타입 : 서로 다른 두 개의 노드와 두 노드를 잇는 하나의 세그먼트로 구성 폴리곤 타입 : n개의 노드와 n개의 세그먼트로 구성 폴리라인 타입 : n개의 노드와 n-1개의 세그먼트로 구성 시공간 데이터 탐색 절차 주소를 행정구역으로 변환 문자열 처리 함수를 이용해 쉽게 변환 가능 주소를 좌표계로 변환 시공간 데이터의 주소를 이용하여 위도와 경도로 변환 지오 코딩 서비스를 이용하여 좌표계로 변환 ex.Geocoder.us (위도 경도 반환 서비스) Google Maps Latitude Popup (구글 매쉬업 프로젝트) Geopy (파이썬용 지오 코딩 도구 모음) 행정구역 및 좌표계를 지도에 표시 시공간 데이터에 따라 행정구역 데이터를 지도에 표시하거나 좌표계를 지도에 표시함 코로플레스 지도 어떤 데이터 수치에 따라 지정한 색상 스케일로 영역을 색칠해서 표현하는 방법으로 등치지역도라고도 함 영역별 데이터를 표현하는 가장 보편적인 방법으로 데이터값의 크기에 따라 지역별로 색을 다르게 표시함 인구밀도가 매우 높은 지역과 낮은 지역에 동일한 척도를 적용할 경우 표시한 지역의 면적이 실제 데이터값의 크기를 반영할 수 없다는 단점 카토그램 특정한 데이터값의 변화에 따라 지도의 면적이 왜곡되는 지도로 변량비례도라고도 함 데이터값이 큰 지역의 면적이 시각적으로도 더 크게 표시됨으로써 데이터값의 크기를 직관적으로 인지할 수 있다는 장점 지도의 형태를 왜곡시킴으로써 데이터 지각의 왜곡을 방지하도록 보정 버블플롯맵 버블차트에 위도 경도 정보를 적용하여 좌표를 원으로 시각화한 지도 원의 크기, 색깔 등을 반영하여 시각화 표현 2. 다변량 데이터 탐색다변량 데이터 변량 : 조사 대상의 특징, 성질을 숫자 또는 문자로 나타낸 값 일변량 분석 vs. 이변량 분석 vs. 다변량 분석 다변량 분석 기법 다변량 데이터 탐색 도구 산점도 행렬 두 변수 간의 산점도를 행렬로 나타내 변수 간의 연관성을 표현한 그래프 그림행렬 변수가 여러 개 있을 때 변수쌍 간의 관계를 보기 위함 개별 Y대 개별 X 산점도 행렬 y축 및 x축 변수를 사용하여 가능한 각 xy 조합의 그래프 생성 별 그림 별 모양의 점을 각각의 변수에 대응되도록 한 뒤 각각의 변숫값에 비례하도록 반경을 나타내어 관찰 값을 그림으로 표시한 것 3. 비정형 데이터 탐색비정형 데이터의 개념 일정한 규격이나 형태를 지닌 숫자 데이터와 달리 이미지와 영상, 텍스트처럼 형태와 구조가 다른 구조화되지 않은 데이터 비정형 데이터의 유형 비정형 데이터의 탐색 방법 비정형 데이터 탐색 플랫폼 구성 예시 텍스트 마이닝 텍스트 마이닝 응용 분야 텍스트 마이닝 용어 소셜 네트워크 분석 소셜 네트워크 분석 방법론 네트워크 구조를 파악하기 위한 요소 - 중심성전체 네트워크에서 한 개체가 중심에 위치하는 정도를 표현하는 지표 네트워크 노드 출처 내용 출처 이지패스 2021 빅데이터분석기사 필기(수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음 https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 2. 빅데이터 분석 기획 (2) 데이터 탐색]- 세우초밥 https://blog.naver.com/ericalee97/222162993681 상관계수에 따른 그래프 유형 https://nittaku.tistory.com/456 Spaceship Titanic data set 그래프 https://www.kaggle.com/code/taehyeon0915/newbie-s-catboost-spaceship-titanic-prediction 위키피디아 &#x2F; 줄기_잎_그림 https://ko.wikipedia.org/wiki/줄기_잎_그림","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"2과목. 빅데이터 탐색 1장. 데이터 전처리","slug":"Study/DataQ/BigData/Class2.1_Data_Preprocessing","date":"2022-09-25T00:00:00.000Z","updated":"2022-09-25T13:06:36.896Z","comments":true,"path":"2022/09/25/Study/DataQ/BigData/Class2.1_Data_Preprocessing/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/25/Study/DataQ/BigData/Class2.1_Data_Preprocessing/","excerpt":"","text":"1장 데이터 전처리데이터 분석에 앞서 데이터 전처리는 반드시 거쳐야 하는 과정. 핵심 키워드 데이터 정제 분석변수 처리 차원의 축소 클래스 불균형 01 데이터 정제1. 데이터 전처리의 이해데이터 전처리 데이터 분석을 위한 필수 과정으로 데이터를 정제한 뒤, 데이터 가고으 통합, 정리, 변환을 통해 데이터 분석 변수를 처리하는 등의 작업으로 데이터 분석 결과의 신뢰도를 높이기 위한 과정 데이터 전처리의 중요성 전처리 결과가 분석 결과에 직접적인 영향을 주기 때문에 전처리는 반복적으로 수행해야 함 데이터 분석의 단계 중 가장 많은 시간이 소요되는 단계가 데이터 수집과 데이터 전처리 단계임 데이터 정제 → 결측값 처리 → 이상값 처리 → 분석 변수 처리 2. 데이터 정제데이터 정제 데이터 결측값 결측값 필수적인 데이터가 입력되지 않고 누락된 값 처리 방법 : 중심 경향값 넣기 (평균값, 중앙값, 최빈값) 분포 기반 처리 데이터 이상값 이상값 데이터의 범위에서 많이 벗어난 아주 작은 값이나 아주 큰 값 처리 방법 : 하한보다 낮으면 하한값 대체 상한보다 높으면 상한값 대체 노이즈 노이즈 실제는 입력되지 않았지만 입력되었다고 잘못 판단한 값 처리 방법 : 일정 간격으로 이동하면서 주변보다 높거나 낲으면 평균값 대체 일정 범위 중간값 대체 특정값 대체법의 종류 평균 대체법 : 평균, 중앙값, 최빈값 등의 대푯값으로 대체하는 방법 단순 확률 대체법 : 평균값으로 대체 시 발생할 수 있는 추정량 표준 오차의 과소 추정 문제를 보완하기 위한 방법 보삽법 : 시계열 자료의 누락된 데이터를 보완하기 위해 사용. 매해 자료를 수집하는 경우, 한 해의 데이터가 결측인 경우 나머지 관측치만을 가지고 평균을 계산하는 방법 평가치 추정법 : 약간의 오차는 감수하면서 원래의 값을 추정하는 방법 다중 대치법 : 결측 데이터가 있는 데이터셋을 결측치 추정을 통해 완벽한 데이터셋으로 생성한 뒤, 결측치가 채워진 데이터셋을 통해 결측치를 추정. 여러 번의 결측치 추정은 오류를 줄이는 데 도움이 되며 기존 데이터의 불확실성을 유지하면서 결과를 얻을 수 있다. 완전정보 최대우도법 : 적합함수인 최대우도를 바탕으로 결측치가 없느 케이스로부터 추정되는 모형모수를 가지고 가중평균을 구성하여 결측치 대신 사용하는 방법 02 분석 변수 처리1. 변수 선택변수 선택 변수 선택법 부분 집합법(All subset) 모든 가능한 모델을 고려하여 가장 좋은 모델을 선정하는 방법 변수가 많아짐에 따라 검증해야 하는 회귀 분석도 만아지는 단점 변수의 개수가 적은 경우 높은 설명력을 가진 결과를 도출해내는데 효과적 ‘임베디드 기법’ 이라고도 하며 라쏘, 릿지, 엘라스틱넷 등의 방법을 사용 단계적 변수 선택방법 전진 선택법 변수의 개수가 많을 때 사용할 수 있지만, 변숫값이 조금만 변해도 결과에 큰영향을 미치기 때문에 안정성이 부족한 방법. 상관계수의 절댓값이 가장 큰 변수에 대해 부분 F 검정으로 유의성 검정을 하고 더는 유의하지 않은 경우 해당 변수부터는 더 이상 변수를 추가하지 않는다. 후진 제거법 다중공선성이 높게 나타난 변수를 하나씩 제거하는 후진제거법을 적용하는 것 다중공선성 일부 설명 변수가 다른 설명변수와 상관정도가 높아 데이터 분석 시 부정적인 영향을 미치는 것을 의미 전체 변수의 정보를 이용한다는 장점 변수의 개수가 너무 많은 경우 적용하기 어렵다 단계적 방법 전진 선택법과 후진 제거법을 보완한 방법 변수를 연속적으로 추가 혹은 제거하면서 AIC가 낮아지는 모델을 찾는 방법 단계적 변수 선택 방법 AIC(Akaike’s Information Criterion) AIC 값이 작을수록 상대적으로 좋은 모델이라고 판단할 수 있다 AIC &#x3D; -2ln(L) + 2K -2ln(L) &#x3D; 모형의 적합도 L &#x3D; 우도값 K &#x3D; 상수항을 포함한 모든 독립변수의 수 AIC는 주어진 데이터에 대한 통계 모델의 상대적인 품질을 평가하는 기준 AIC를 최소화한다는 것은 우도를 가장 크게 하는 동시에 변수의 수는 가장 적은 최적의 모델을 의미 2. 차원 축소차원축소 LDA는 투영을 통해 가능한 클래스를 멀리 떨어지게 하므로 SVM 같은 다른 분류 알고리즘을 적용하기 전에 차원을 축소하는데 자주 사용된다. LDA는 데이터를 최적으로 분류하여 차원을 축소, PCA는 데이터를 최적으로 표현하는 관점에서 데이터를 축소하는 방법 LDA를 적용할 때 베이지안 정리(Bayes’ Theorem)를 활용 베이지안 정리(선형판별분석에서는 판별함수라고 칭함) 사전확률 (P(A))로부터 사후 확률(P(A|B))을 구하는 것 친밀도 : t-SNE 과정에서 측정한 거리를 기준으로 t-분포의 값 SVD 일반적으로 정방행렬에 대해서는 고윳값 분해를 적용 직사각 행렬에 대해서는 고윳값 분해를 이용할 수 없음 데이터 압축 등의 많은 분야에서 활용 U &#x3D; M * M^T를 고윳값 분해해서 얻은 직교행렬 V &#x3D; M^T * M을 고윳값 분해해서 얻은 직교행렬 행렬 U와 V에 속한 벡터는 특이벡터(Singular Vector) 서로 직교하는 성질 ∑ &#x3D; 대각행렬 대각성분은 M * M^T과 M^T * M의 고윳값에 루트를 씌운 값으로 구성 대각에 위치한 값만 존재하고 나머지 위치의 값은 모두 0인 행렬 ∑ 행렬에서 0이 아닌 값들이 바로 특잇값 특잇값의 개수는 행렬의 열과 행의 개수 중 작은 값과 같다. 고윳값과 고유벡터 정방 행렬 A에 대해 다음 식을 만족하는 영벡터가 아닌 벡터 v, 실수 λ를 찾을 수 있다고 가정하자. 위 식을 만족하는 실수 λ를 고윳값(eigenvalue), 벡터 v를 고유벡터(eigenvector)라고 한다. 고윳갑과 고유벡터를 찾는 작업을 고유분해(eigen-decomposition) 또는 고윳값 분해(eigenvalue decomposition)라고 한다. 행렬 A의 고유벡터는 행렬 A를 곱해서 변환을 해도 방향이 바뀌지 않는 벡터 Python을 통한 SVD 예시 3. 파생변수 생성파생변수 기존 변수들을 조합하여 새롭게 만들어진 변수를 파생변수라 한다. 분석가의 주관이 포함 논리적 타당성을 충분히 고려해 생성 파생변수 생성 방법 하나의 변수에서 정보를 추출해 새로운 변수를 생성 주민등록번호에서 나이와 성별을 추출 한 레코드의 값을 결합하여 파생변수를 생성 키와 몸무게를 이용해 BMI 지수라는 변수를 생성 조건문을 이용해 파생변수를 생성 기준값을 정하고 조건문을 통해 BMI 지수에 따라 저체중, 정상 체중, 과체중을 구분한 파생변수를 생성 R로 파생변수 만들기 4. 변수 변환변수 변환 5. 클래스 불균형(불균형 데이터 처리) 어떤 데이터에서 각 클래스가 가지고 있는 데이터의 양에 큰 차이가 있는 경우 불균형 데이터를 사용하여 모델링을 할 경우 관측치 수가 많은 데이터를 중심으로 학습이 진행되기 때문에 관측치가 적은 데이터에 대한 학습은 제대로 이루어지지 않을 가능성이 크다. 클래스 불균형 문제 해결 방법 과소표집(Under Sampling) 무작위로 정상 데이터를 일부만 선택해 유의한 데이터만 남기는 방법 정상 데이터 800개, 페이크 데이터 100개 → 정상 데이터를 제거해 100개로 감소 과대표집(Over Sampling) 사전에 정해진 기준 또는 기준 없이 무작위로 소수 데이터를 복제하는 방법 정상 데이터 500개, 페이크 데이터 20개 → 페이크 데이터를 복제하여 500개로 증가 SMOTE(Synthetic Minority Oversampling Technique) 다수 클래스를 샘플링하고 기존의 소수 샘플을 보간하여 새로운 소수 인스턴스를 합성해내는 방법. 알고리즘을 통해 소수 클래스에 새로운 데이터를 생성 소수 클래스의 데이터 하나를 찾고 해당 데이터와 가까운 K개의 데이터를 찾은 후 주변 값을 기준으로 새로운 데이터를 생성 소수 클래스 수는 다수 클래스의 수와 동일해지게 된다. 클래스 불균형 출처 내용 출처 이지패스 2021 빅데이터분석기사 필기(수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음 https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 2. 빅데이터 분석 기획 (1) 데이터 전처리]- 세우초밥 https://blog.naver.com/ericalee97/222162993681 베이지안 정리 http://egloos.zum.com/posterior/v/9604165 고유값 분해 https://datascienceschool.net/02 mathematics/03.03 고윳값 분해.html","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"1과목. 빅데이터 분석 기획 3장. 데이터 수집 및 저장 계획","slug":"Study/DataQ/BigData/Class1.3_Data_collection_and_storage_plan","date":"2022-09-24T00:00:00.000Z","updated":"2022-09-26T08:43:26.080Z","comments":true,"path":"2022/09/24/Study/DataQ/BigData/Class1.3_Data_collection_and_storage_plan/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/24/Study/DataQ/BigData/Class1.3_Data_collection_and_storage_plan/","excerpt":"","text":"3장. 데이터 수집 및 저장 계획 이 장은 데이터를 분석하기 전 단계의 과정을 다루고 있다. 핵심키워드 데이터의 유형 데이터의 변환 데이터 저장과 처리 하둡 에코시스템 01_ 데이터 수집학습목표 데이터를 수집하고 분석에 활용할 수 있도록 변환하는 과정을 학습한다 데이터의 품질을 검증하는 방법을 학습한다. 1. 데이터 수집의 이해와 수집 방법데이터 수집에서 고려할 사항 데이터의 위치 데이터의 주기 데이터의 수집 방법 데이터의 저장 형태 데이터를 수집하는 절차 데이터 선정 : 데이터 선정은 데이터 분석 결과와 품질에 큰영향을 미친다. 데이터 선정 시 우선 고려할 사항은 수집 가능성이다. 수집 세부 계획 수립 : 데이터 위치와 유형을 파악하고 그에 맞는 준비를 해야 한다. 데이터의 위치가 시스템 내부 혹은 외부에 있는지를 파악하고 그에 맞는 기술 및 보안 정책 등을 파악해야 한다. 테스트 수집 실행 : 수집 가능성, 보안 문제, 정확성 등을 만족하는지 검증하고 실제 활용 가능성까지 검토해야 한다. 데이터 유형에 따른 데이터 수집 방법 데이터 유형 데이터 종류 데이터 수집 방법 정형 데이터 DBMS, 스프레드시트 ETL, FTP, Open API 반정형 데이터 HTML, XML. JSON, 웹 문서, 웹 로그, 센서 데이터 웹 크롤링, RSS, Open API, FTP 비정형 데이터 소셜 데이터, 문서, 이미지, 오디오, 비디오, IoT 웹 크롤링, RSS, Open API, Streaming, FTP 2. 데이터 유형 및 속성 파악정형 데이터와 비정형 데이터 정형 데이터(Structured Data) - 고정된 구조로 정해진 필드에 저장된 데이터- 엑셀 스프레드시트, RDBMS(관계형 데이터베이스), CSV 파일 형태- 데이터로서 활용성이 가장 높다. 반정형 데이터(Semi-Structured Data) - 고정된 필드에 저장되어 있지는 않지만, 데이터와 메타데이터, 스키마 등을 포함하는 데이터- XML,HTML,JSON- 규칙을 가지고 있어 필요 시 정형 데이터로 변형이 가능하다. 비정형 데이터(Unstructured Data) - 미리 정해진 구조가 없고, 고정된 필드에 저장되어 있지 않은 데이터- 동영상, 소셜 네트워크 댓글 같은 문자 데이터, 위치 데이터, 오디오 데이터 등- 비정형 데이터는 크기가 크고 규칙이 없어 복잡하다- 데이터 변환 기술의 발전으로 비정형 데이터의 활용성과 공유 가능성이 높아지고 있다. 내부 데이터와 외부 데이터 3. 데이터 변환데이터 변환 방법 평활화(Smoothing) 데이터로부터 발생할 수 있는 잡음을 제거하기 위해 추세에 맞니 않는 이상값들을 제거하여 데이터를 변환하는 방법 집계(Aggrengation) 그룹화 연산을 데이터에 적용하여 데이터를 요약하는 방법 일반적으로 다중 추상 레벨에서 데이터 분석을 위한 데이터 큐브 생성에 사용 예를 들면 매일 발생하는 데이터를 월별 또는 연도별로 그룹화하는 총계를 계산해 요약하는 조치를 취함 일반화(Generalization) 특정 구간에 분포하는 값으로 스케일(규모)을 변화시키는 방법 데이터를 특정 범위 내의 값으로 축소하는 것을 의미 예측 모델이 새로운 자료에 얼마나 잘 적용되는가를 의미 최솟값과 최댓값의 편차가 크거나 다른 열보다 데이터가 지나치게 큰 열이 있을 때 주로 사용 일반화는 데이터 마이닝 기법의 비정형성을 어느 정도 해결하고 보완하는 데 도움 정규화(Normalization) 데이터를 특정 구간 안에 들어가게 이상값을 변환하는 방법 최단 근접 분류와 군집화 같은 거리 측정 등에 특히 유용 구체적인 방법으로는 최소-최대 정규화, z-score 정규화, 소수 스케일링 정규화 정규화의 종류 최소-최대 정규화 : 원본 데이터에 대해 선형 변환을 수행함으로써 정규화하는 방법으로, 원본 데이터 값들의 관계를 그대로 유지하는 방법 z-score 정규화 : 평균값과 표준편차를 기초로 하여 정규화하는 방법으로, 실제로 최솟값과 최댓값이 알려져 있지 않거나 최소-최대 정규화에 큰 영향을 주는 이상치가 있는 경우에 이용할 수 있는 방법 소수점 변경 정규화 : 소수점을 변경하여 정규화하는 방법으로, 정규화된 값의 절댓값은 1보다 작아야 한다. 범주화 데이터 통합을 위해 상위 레벨 개념의 속성이나 특성을 이용해 일반화하는 방법 상위 개념으로 통합하는 것을 의미(나이 : 30대, 40대&#x2F; 까치, 까마귀 : 새, 동물) 이산화 : 연속형 변수를 이산 변수로 변환하는 방법 이산화 방법은 인접 구간의 처리 방법에 따라 분할 이산화, 병합 이산화로 나누거나 클래스 정보 활용 여부에 따라 지도 이산화, 비지도 이산화로 나눌 수 있다. 이진화 : 연속형과 이산형 속성을 한 개 이상의 이진 속성으로 변환하는 것 데이터 축소 · 차원 축소 데이터 축소 같은 정보량을 가지면서 데이터의 크기를 줄이는 방법이다. 대규모 데이터를 다루는 경우 데이터 분석의 효율성을 높이기 위해 필요한 데이터 변환 과정이다. 차원 축소 차원 축소를 통해 데이터 잡음을 제거할 수 있으며 데이터셋을 더 다루기 쉽게 만들어 준다는 장점이 있다. 여러 속성 중 분석하는 데 관계없거나 중복되는 속성을 제거하는 작업을 통해 속성의 최소 집합을 찾아내는 방법이다. 데이터 압축 데이터 인코딩이나 변환을 통해 데이터를 축소하는 방법이다. 아무런 손실 없는 압축기법을 무손실압축기법(Lossless)이라고 하며, 대표적인 예로는 BMP 포맷이 있다. 데이터의 손실이 있는 경우에는 이를 손실압축기법(Lossy)이라고 하고, 대표적인 예로는 JPEG 포맷이 있다. 4. 데이터 품질 검증데이터 품질 검증 요소데이터 품질 검증 요소에는 데이터 값 검증, 데이터 구조 검증, 데이터 관리 프로세스 검증 데이터값 검증 정형 데이터 정형 데이터의 데이터 값 검증은 대상이 되는 데이터베이스 테이블, 칼럼, 관계, 업무 규칙을 기준으로 데이터 값을 관리하고 분석. 또 데이터 값 품질 기준에 따라 오류를 검출하고 개선안을 제공. 비정형 데이터 비정형 데이터의 데이터 값 검증은 비정형 콘텐츠 자체의 상태와 메타데이터에 대한 품질 검증으로 구분. 비정형 콘텐츠의 유형에 따라 시각, 청각, 자동화된 도구를 이용해 분석하기 때문에 각각의 품질 기준이 상이 메타데이터에 대한 품질 검증은 정형 데이터 품질 검증 방법으로 검증하고 나아가 데이터가 그 콘텐츠를 정확히 식별할 수 있는지, 즉 데이터와 콘텐츠 개체 간의 일치하는 정도까지 무결성의 개념이 확장된다. 데이터 구조 검증 데이터 구조 검증은 데이터 모델링 관점에서 이루어진다. 데이터 관리 프로세스 검증 데이터를 관리하는 절차, 인력, 조직을 분석해 문제점을 발견하고 개선할 수 있는 핵심 업무 프로세스를 표준화해 재설계한다. 02_ 데이터의 저장과 처리학습목표 데이터의 저장과 처리 방법, 그리고 빅데이터 플랫폼에 대해 자세히 학습한다. 여기서는 가장 널리 활용되는 하둡 플랫폼을 학습하기로 한다. 1. 데이터 저장과 처리2. 데이터 저장과 처리를 위한 플랫폼스타 스키마 장단점 장점 복잡도가 낮아서 이해하기 쉽다.쿼리 작성이 용이하고 조인의 테이블 수가 적다. 단점 차원 테이블들의 비정규화로 데이터 중복이 발생하여 상대적으로 데이터 적재에 시간이 많이 소요된다. 스노우플레이크 스키마 장단점 장점 데이터 중복이 제거되어 데이터 적재 시 상대적으로 데이터 적재 소요 시간이 빠르다. 단점 복잡성이 증가해 조인 테이블의 개수가 증가한다.쿼리 작성의 난도가 증가한다. ETL 프로세스데이터 원천으로부터 데이터를 추출 및 변환하여 운뎡 데이터 스토어(ODS : Operational Data Store), 데이터 웨어하우스(Data Warehouse), 데이터 마트(Data Mart)에 데이터를 적재하는 작업 STEP 1 Interface 다양한 이기종 DBMS 및 스프레드시트 등 데이터 원천으로부터 데이터를 획득하기 위한 인터페이스 메커니즘 구현 STEP 2 Staging ETL 수립된 일정에 따라 데이터 원천(Source)으로부터 트랜잭션 데이터 획득 작업 수행 후, 획득된 데이터를 스테이징 테이블에 저장 STEP 3 Profiling ETL 스테이징 테이블에서 데이터 특성을 식별하고 품질을 측정 STEP 4 Cleansing ETL 다양한 규칙을 활용해 프로파일링된 데이터 보정 작업 STEP 5 Integration ETL 이름, 값, 구조 등 데이터 충돌을 해소하고 클렌징된 데이터를 통합 STEP 6 Denormalization ETL 운영 보고서 생성, 데이터 웨어하우스 또는 데이터 마트 데이터 적재를 위해 데이터 비정규화 수행 ETL과 CDC의 차이 ETL CDC 공통목적 원천 데이터를 DW, DM 등에 적재 원천 데이터를 DW, DM 등에 적재 특징 실시간이 아닌 정해진 시점의 완료된 데이터를 적재 실시간(Real Time) 혹은 준실시간(Near Real Time)으로 적재 용도 예 거래 집계, 일일 회계 집계, 원장 등이벤트 단위가 아닌 소스 데이터 취합 용도 이상 감지 경보, 금융 거래 이상 경보변경된 이벤트 감지 용도 기술 변경 데이터만 적재 혹은 All Copy(원천 테이블 사용) 변경 이력을 관리하는 DB Archive Log (원천 테이블이 아닌 Archive Log 시스템 테이블 사용) 적재 수준 적재 시점의 적재 수준(시, 일, 월 등) 시점에 관계없이 모든 원천 데이터의 변경 로그 기록을 적재 ODS 프로세스ODS는 데이터에 추가 작업을 하기 위해 다양한 원천 데이터로부터 데이터를 추출 통합한 데이터베이스 STEP 1 Interface - 다양한 데이터 원천으로부터 데이터를 획득하는 단계다.- 데이터 원천은 관계형 데이터베이스, 스프레드시트, 플랫 파일, 웹 서비스, 웹 사이트, XML 문서 또는 트랜잭션 데이터를 저장하고 있는 모든 알려진 데이터 저장소(Repository) 등이다. 프로토콜 : - OLEDB(Object Lonking and Embedding Database)- ODBC(Object Data Base Connectivity) STEP 2 Data Staging - 이단계에서는 작업 일정이 통제되는 프로세스에 의해 데이터 원천으로부터 트랜잭션 데이터가 추출되어 하나 또는 그 이상의 스테이징 테이블에 저장된다.- 이 테이블들은 정규화가 배제되며, 테이블 스키마는 데이터 원천의 구조에 의존적이다. 데이터 원천과 스테이징 테이블과 데이터 매핑은 일대일 또는 일대다로 구성 될 수 있다. STEP 3 Data Profiling - 이단계에서는 범위 도메인 유일성 확보 등의 규칙을 기준으로 다음과 같은 절차에 따라 데이터 품질 점검을 한다.- 선행 자료 또는 조건: 데이터 프로파일링 요건 - Step 1 : 데이터 프로파일링 수행 - Step 2 : 데이터 프로파일링 결과 통계 처리 - Step 3 : 데이터 품질 보고서 생성 및 공유 STEP 4 Data Cleansing - 이 단계에서는 클렌징 ETL 프로세스로 앞 데이터 프로파일링 단계에서 식별된 오류 데이터를 다음 절차에 따라 수정한다. - 선행 자료 또는 조건 : 데이터 품질 보고서, 데이터 클렌징 요건 - Step 1 : 클렌징 스토어드 프로시저 실행(예비 작업) - Step 2 : 클렌징 ETL 도구 실행 STEP 5 Data Integration - 이 단계에서는 앞 단계에서 수정 완료한 데이터를 ODS 내의 단일 통합 테이블에 적재하며, 다음의 단계를 거친다. - 선행 자료 또는 조건 : 데이터 클렌징 테이블, 데이터 충돌 판단 요건 - Step 1 : 통합 스토어드 프로시저 실행(예비 작업) - Step 2 : 통합 ETL 도구 실행 STEP 6 Data Export - 앞 단계에서 통합된 데이터를 익스포트 규칙과 보안 규칙을 반영한 익스포트 ETL 기능을 수행해 익스포트 테이블을 생성한다. - 다양한 전용 DBMS 클라이언트 또는 데이터 마트, 데이터 웨어하우스에 적재한다. 해당 데이터는 OLAP 비정형 질의에 활용될 수 있다. 데이터 웨어하우스와 데이터 레이크 비교 속성 데이터 웨어하우스 데이터 레이크 스키마 - Schema-on-write - Schema-on-read 액세스 방법 - 표준화된 SQL 및 BI 도구를 통해 액세스 - SQL과 유사한 시스템(NO.SQL, H-Base, Mongo-DB 등) - 개발자가 만든 프로그램(Spark, Map Reduce, YARN, Presto 등)을 통해 액세스 데이터 - 정제된 데이터 - 로 데이터 비용 - 저장 및 처리에 높은 비용 - 저렴한 비용으로 가능 특징 - 빠른 응답시간- 간편한 데이터 사용- 간편한 데이터 사용- 성숙한 거버넌스 체계- 데이터 접근성이 제한적- 정제되고 안전한 데이터- 높은 동시성과 통합성 - 빠른 응답시간- 간편한 데이터 사용- 성숙한 거버넌스 체계- 데이터 접근성이 매우 높음- 단일 데이터 모델로부터 자유로움- 저장 용량의 확장성이 좋음- 도구의 확장성이 좋음- 리얼타임 데이터 분석 가능 - 스트리밍 데이터 처리- 단일 소스에서 정형&amp;비정형 데이터 사용 가능- 사용자가 응용프로그램 및 쿼리를 커스터마이징해서 사용 가능- 민첩한 모델링 지원- 빅데이터 분석 솔루션과 연동이 편리 3. 하둡 플랫폼에서의 데이터 저장과 처리하둡 에코시스템 하둡 에코시스템 - 수집 및 연결 프레임워크 하둡 에코시스템 - 분석 및 관리 프레임워크 하둡 에코시스템 - 관리 프레임워크 출처 내용 출처 이지패스 2021 빅데이터분석기사 필기(수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음 https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 1. 빅데이터 분석 기획 (3) 데이터 수집 및 저장 계획]- 세우초밥 https://blog.naver.com/ericalee97/222162993681","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"논문 BERT 리뷰","slug":"Study/ML_DL/BERT _review","date":"2022-09-23T00:00:00.000Z","updated":"2022-09-23T14:38:48.305Z","comments":true,"path":"2022/09/23/Study/ML_DL/BERT _review/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/23/Study/ML_DL/BERT%20_review/","excerpt":"","text":"BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding Review Conference : NAACL Link : BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Year : 2018 저자 : Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Github : https://github.com/google-research/bert 동영상 : https://vimeo.com/365139010 BERT란?Bidirectional Encoder Representations from Transformers. Unlike recent language representation models. BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine tuned with just one additional output layer to create state-of-the art models for a wide range of tasks, such as question answering and language inference, without substanial task specific architecure modifications. 핵심 요약 trasformer의 encoder network를 기반으로, self-attention을 이용하여 bidirectional하게 언어 특성을 학습합니다. transformer란? transformer는 the mechanism of self-attention을 채택하는 딥 러닝 모델로서, 입력 데이터의 각 부분의 중요성을 차등적으로 가중시킨다. 주로 자연어 처리(NLP)와 컴퓨터 비전(CV) 분야에서 사용된다. transformer background before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers are built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention. transformer 이전에 대부분의 최첨단 NLP 시스템은 LSTM과 게이트 반복 유닛(GRU)과 같은 게이트 RNN에 의존했으며 주의 메커니즘이 추가되었다. 변압기는 RNN 구조를 사용하지 않고 이러한 주의 기술을 기반으로 구축되어 주의 메커니즘만으로도 RNN의 성능을 주의와 일치시킬 수 있다는 사실을 강조한다. Encoder Each encoder consists of two major components : a self-attention mechanism and a feed-forward neural network. The self-attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings. 각 인코더는 self-attention mechanism과 feed-forward neural network 이라는 두 가지 주요 구성 요소로 구성된다. 자체 주의 메커니즘은 이전 인코더로부터 입력 인코딩을 받아들이고 출력 인코딩을 생성하기 위해 서로 관련성을 평가한다. 참조 : Transformer (machine learning model) - Wikipedia Attention Is All You Need 참조 : https://www.youtube.com/watch?v=mxGCEWOxfe8 MLM(Masked language model)과 NSP(next sentence prediction)등의 pre-training방법을 제시하였습니다. MLM In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. deep bidirectional representation을 훈련시키기 위해 입력 토큰의 일부 비율을 무작위로 마스킹한 다음, 마스킹된 토큰을 예측한다. 참조 : NSP pre-training방법으로 feature representation을 학습한 뒤 fine-tuning만으로 down-stream task를 수행합니다. Introduction &amp; Related Work저자는 먼저 사전 훈련을 통해 자연어 처리 Task의 성능을 향상 시키는 PLM(pre-trained language model)의 사례를 제시합니다. Feature-based 예시)ELMo : pre-trained representation 을 추가 특성으로 사용하여 Task에 특화된 모델 구조를 설계합니다. fine-tuning 예시) GPT : task에 특화된 파라미터를 최소화 하고, 사전 학습된 파라미터 전부를 fine-tuning 합니다. 두 방식 모두 pre-training에서 동일한 objective를 사용하고, unidirectional language model을 가정합니다. 이 때문에, 문맥을 단 방향으로만 습득하게 되어, 양방향의 문맥 이해가 필요한 문답 task 등의 성능을 보장하지 못함을 지적합니다. 이에 논문은, fine-tuning 방식을 개선하는 PLM으로 BERT(Bidirectional Encoder Representations from Transformers) 구조를 제안합니다. BERT의 pre-training은 masked language model(MLM) 과 next-sentence-prediction task(NSP)를 통해 양방향 문맥을 학습하게 됩니다. BERT 출력 계층을 제외하고, 사전 훈련과 미세 조정 모두에서 동일한 아키텍처가 사용된다. 동일한 사전 훈련된 모델 매개 변수는 다양한 다운스트림 작업에 대한 모델을 초기화하는 데 사용된다. 미세 조정 중에는 모든 파라미터가 미세 조정됩니다. [CLS]는 모든 입력 예제 앞에 추가된 특수 기호이고 [SEP]는 특수 구분 토큰(예: 질문&#x2F;답변 구분)입니다. BERT는 크게 pre-training 단계와 fine-tuning 단계, 두가지 단계로 구분하며, 각 단계는 모두 동일한 모델 구조를 공유합니다. Pre-training 단계에서는 레이블링 하지 않는 데이터를 기반으로 학습을 진행합니다. Fine-tuning에서는 pre-trained 파라미터로 초기화된 모델을 레이블링된 데이터로 학습합니다. Model ArchitectureBERT의 구조는 다층레이어로 구성된 양방향 Transformer의 Encoder를 기본으로 하며, 아래와 같은 표기로 모델 속성을 나타내었습니다. L : Layer 갯수 (ex : Transformer Block) H : Hidden size A : self-attention head 의 갯수 논문에서는 크게 두 모델을 제시합니다. BERT-BASE : L &#x3D; 12, H &#x3D; 768, A &#x3D; 12, 총 파라미터 수 &#x3D; 110M (GPT 와 동일) BERT-LATGE : L &#x3D; 24, H &#x3D; 1024, A &#x3D; 16, 총 파라미터 수 &#x3D; 240M Input&#x2F;Output Representations실제 사용하는 다양한 task에 사용할 수 있도록, 단일 문장과 쌍으로 이어진 문자을 모두 하나의 Sequence 로 표현합니다. 단어 임베딩으로는 WordPiece embedding 을 사용하며 30,000개의 token vocalbulary를 사용합니다. Input은 Token embedding + Segment embedding + Position embedding 으로 구성됩니다. Token Embedding Sequence 의 첫 토큰은 [CLS]토큰을 사용합니다. 두 문장이 이어진 경우, [SEP]토큰으로 문장을 구분하며, 마지막에도 [SEP] 토큰을 추가합니다. [CLS]는 모든 입력 예제 앞에 추가된 특수 기호이고 [SEP]는 특수 구분 토큰(예: 질문&#x2F;답변 구분)입니다. Segment Embedding 두 문장이 있을 때, 각각의 문장에 sentence A &#x2F; sentence B 임베딩을 적용합니다. Positional Embedding Transformer 의 삼각함수 Encoding 이 아닌, lookup table 에서 각 position의 vector들을 찾아서 Position 을 embedding합니다. Pre-training BERT전통적인 left-to-right &#x2F; right-to-left LM 을 사용해서 pre-train 하는 ELMo, GPT와는 다르게, BERT는 2개의 unsupervised task를 이용해서 사전학습을 수행합니다. Task #1 : Masked LM기존의 언어 모델을 Bidirectional 하게 처리하는 경우, 간접적으로 예측하려는 단어를 참조할 수 있게 되어 예측 자체가 무의미해질 수 있음을 지적합니다. BERT는 전체 Sequence 에서 15%의 토큰을 가리는 Mask 를 추가하여 양방향 학습의 문제를 해결하였습니다. 또한, 사전 훈련 모델이 [MASK] 토큰을 사용하지 않는 Fine-tuning에도 적용될 수 있도록 하는 Masking Rule을 제시합니다. 80%는 [MASK]로 치환 1my dog is hairy -&gt; my dog is [MASK] 10%는 랜덤한 토큰으로 치환 1my dog is hairy -&gt; my dog is apple 10%는 기존의 토큰을 그대로 사용 1my dog is hairy -&gt; my dog is hairy 이로 인해 model은 단어의 기원(원본&#x2F;MASK&#x2F;random changed)을 알지 못한 채 모든 input token에 대해서 distributional contextual representation을 유지하게 됩니다. 또한 전체의 1.5% 토큰만이 랜덤하게 변경되었기에, 모델이 잘못 학습될 우려 또한 적음을 제시합니다. 이러한 비율은 위의 실험을 통해 제시되었습니다. 전체적으로 fine-tuning task 가 feature-based 보다 좋은 성능을 보임 특정 Masking Rule 만을 사용하는 것이 아닌, Rule 혼합이 더 좋은 성능을 보임 최종적으로는 cross-entropy loss 를 사용해서 기존의 토큰을 예측하도록 학습합니다. Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding he relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the entences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence prediction (NSP). 💡 문장 관계를 이해하는 모델을 훈련하기 위해, 우리는 모든 단일 언어 말뭉치에서 사소한 것으로 생성될 수 있는 이항화된 다음 문장 예측 작업을 위해 사전 훈련한다. 구체적으로, 각 사전 훈련 예에 대한 문장 A와 B를 선택할 때, 시간의 50%는 A를 따르는 실제 다음 문장(IsNext로 레이블링됨)이고, 50%는 말뭉치(NotNext로 레이블링됨)의 무작위 문장이다. 그림 1에서 알 수 있듯이, C는 다음 문장 예측(NSP)에 사용된다. Question-answering(QA), Natural Language Interference(NLI) 등의 task는 두 문장 사이의 관계를 이해해야 하는 task입니다. 일반적인 LM 으로는 해당 Task 의 학습이 어려우므로, 이 또한 학습을 진행합니다. 두 문장과 레이블로 구성된 다음의 데이터 셋으로 Binary 분류 문제를 학습합니다. 50% : 실제로 이어진 두 문장을 제시 : 레이블 IsNext 50% : 관계가 없는 임의의 문장을 제시 : 레이블 NotNext Pre-training data사전 훈련을 위해 사용한 corpus 를 제시합니다. BooksCorpus (800M words) English Wikipedia (2,500M words) : text passage 만 사용했고, 목록이나 표 등은 제외하여 사용 긴 문맥을 학습하기 위해서 Billion Word Benchmark 와 같이 섞인 문장으로 구성된 데이터는 사용하지 않았습니다. Fine-tuning BERTTask 별 입력의 개수(단일 문장, 2개의 문장)에 따라 하나의 sequence 를 생성하여 입력으로 사용합니다. 이후, 파라미터들을 해당 task에 맞게 end-to-end로 업데이트합니다. Sequence tagging 이나 question answering 같이 token-level task 들의 경우, 마지막 transformer layer의 token 들을 사용하여 fine-tuning 합니다. Sentence Classification, sentiment analysis 등의 sentence-level classification task 들은 마지막 layer의 CLS token의 hidden state를 fine-tuning에 이용합니다. Pre-training과 비교했을 때, fine-tuning 은 빠르게 학습이 가능합니다. ExperimentsBERT fine-tuning을 이용한 11개의 NLP task의 결과를 제공합니다. 모든 Task 에서 State-Of-Arts 를 달성하였고, 각각의 특성에 맞는 학습 방법을 제시합니다. GLUEGLEU benchmark는 다양한 natural language understanding task를 위한 문장 분류 Task 입니다. BERT 모델에 분류를 위한 classification layer를 추가합니다. SQuAD v1.1SQuAD v1.1 dataset은 Question Answering dataset으로, 질문과 지문의 형태 주어진 데이터에서 답변을 찾는 과제입니다. BERT 는 질문과 지문을 하나의 single sequence 로 묶어서 input으로 만든 뒤, 지문에서 정답이 될 수 있는 영역을 찾는 방식으로 Task 를 전환해 학습합니다. SQuAD v2.0SQuAD v2.0 은 1.1 과 유사하지만, 지문만으로는 대답이 불가능한 질문이 포함된 dataset 입니다. BERT 는 대답이 불가능한지 여부를 CLS token 을 이용해 분류하는 문제를 추가합니다. SWAGThe Situations With Adversarial Generations (SWAG) dataset은 앞 문장이 주어졌을 때, 보기로 주어진 4 문장 중 가장 잘 어울리는 문장을 찾는 task 입니다. Fine-tuning 을 위해, 앞 뒤 문장을 조합해 4개의 문장을 생성하고, 해당 sequence 의 정답 여부를 Classification 하는 Task 를 수행합니다. Ablation StudiesEffect of Pre-training Tasks 사전 훈련의 효과를 확인하기 위해, 훈련을 제거하며 실험한 결과를 제시합니다. No NSP : masked LM(MLM) 으로만 학습되고 NSP는 사용하지 않는 경우 NLI 문제에서 성능이 하락 NSP 가 문장간의 논리적인 구조 파악에 중요한 역할을 수행함을 시사 LTR &amp; No NSP : MLM이 아닌 Left-To-Right model 을 사용하고 NSP도 사용하지 않는 경우 모든 task에 대해서 성능이 감소하며, 특히 MRPC와 SQuAD 에서 큰 폭의 성능 저하를 확인 LTR &amp; No NSP + BiLSTM : BiLSTM 을 추가하여 양방향성을 제공 BiLSTM 이 없는 경우에 비해 미미하게 성능이 상승하지만, MLM 에 비해서는 떨어짐 MLM이 bi-directionality 가 더 강함을 시사 Effect of Model size 모델의 크기가 fine-tuning 정확도에 어떠한 영향을 주는지를 확인합니다. 모델이 커질수록, Pre-training 의 정확도가 상승 downstream task 가 작은 스케일의 데이터셋을 사용할 때에도 성능이 상승 Feature-based Approach with BERTBERT를 ELMo 와 같은 feature-based 로 사용하는 방식과 그 장점을 제시합니다. task-specific model 의 추가로 Transformer encoder 만으로 수행 불가능한 task에 적용 가능 Update parameter 감소로 학습 비용 절감 CoNLL-2003 Named Entity Recognition task 로 성능을 평가합니다. BERT 레이어의 일부 activation 에 Bi-LSTM을 부착시켜 해당 레이어만 학습시키는 방식으로 Feature-based approach 를 구현하였고, 기존 ELMo 에 비해서도 좋은 성능을 보입니다. 또한 Fine-Tuning 방법만을 사용하였을 때도, SOTA 에 근접한 결과를 얻었음을 제시합니다. 참조 : https://www.youtube.com/watch?v=30SvdoA6ApE Conclusion RNN은 순차적으로 계산한다면 Transformer는 한번에 계산. BERT는 Transformer를 이용해서 양방향의 문맥을 숫자의 형태로 바꿔주는 딥러닝 모델이다.","categories":[{"name":"논문리뷰","slug":"논문리뷰","permalink":"http://eomtaehyeon.github.io/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"},{"name":"NLP","slug":"논문리뷰/NLP","permalink":"http://eomtaehyeon.github.io/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/"}],"tags":[{"name":"논문리뷰","slug":"논문리뷰","permalink":"http://eomtaehyeon.github.io/tags/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://eomtaehyeon.github.io/tags/NLP/"},{"name":"NAACL","slug":"NAACL","permalink":"http://eomtaehyeon.github.io/tags/NAACL/"},{"name":"BERT","slug":"BERT","permalink":"http://eomtaehyeon.github.io/tags/BERT/"},{"name":"Transformers","slug":"Transformers","permalink":"http://eomtaehyeon.github.io/tags/Transformers/"}],"author":"엄태현"},{"title":"1과목. 빅데이터 분석 기획 2장. 데이터 분석 계획","slug":"Study/DataQ/BigData/Class1.2_Data_Analysis_Plan","date":"2022-09-22T00:00:00.000Z","updated":"2022-09-26T08:44:04.177Z","comments":true,"path":"2022/09/22/Study/DataQ/BigData/Class1.2_Data_Analysis_Plan/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/22/Study/DataQ/BigData/Class1.2_Data_Analysis_Plan/","excerpt":"","text":"2장. 데이터 분석 계획 빅데이터 분석의 전체 프로세스를 이해해야 한다. 특히 프로세스의 순서와 해당 단계에서의 목적을 분명히 이해해야 한다. 핵심키워드 분석 로드맵 데이터 분석 모델링 분석 요건 정의 분석 작업 계획 01_분석 방안 수립학습목표 데이터 분석을 위한 전 단계로 일종의 설계도를 그리는 과정이다. 데이터 분석 방안을 수립하는 과정에 대해 알아본다. 1. 분석 로드맵 설정분석 로드맵 개념 단계별로 추진하고자 하는 목표를 명확히 정의하고, 선 후행 단계를 고려해 단계별 추진내용을 정렬함. 분석 로드맵 단계 단계 추진과제 추진목표 데이터 분석체계 도입 - 분석 기회 발굴 - 분석 과제 정의 - 로드맵 수립 - 비즈니스 약점이 무엇인지 식별 - 분석 과제를 정의하고 로드맵 수립 데이터 분석 유효성 검증 - 분석 알고리즘 설계 - 아키텍쳐 설계 - 분석 과제 파일럿 수행 - 분석 과제에 대한 파일럿 수행 - 유효성, 타당성을 검증 - 기술 실현 가능성을 검증 - 분석 알고리즘 및 아키텍처 설계 데이터 분석 확산 및 고도화 - 변화 관리 - 시스템구축 - 유관 시스템 고도화 - 검증된 분석 과제를 업무 프로세스에 내재화하기 위한 변화관리 실시 - 빅데이터 분석, 활용 시스템 구축 및 유관 시스템 고도화 빅데이터 분석 기획의 능력 단위 요소 분석 대상과 그 방법에 따른 4가지 분석 주제 5단계 빅데이터 분석 방법론 분석 과제 발굴 방법론 개념도 하향식 분석 과제 발굴 방법론 분석 과제가 정해져 있고 이에 대한 해법을 찾기 위해 체계적으로 분석하는 방법 A. 문제 탐색 단계 : 문제를 탐색하는 단계로 크게 4가지 방법 중 하나 혹은 다수를 사용한다. 비즈니스 모델 탐색 기법 업무, 제품, 고객 단위로 비즈니스 문제를 발굴하고, 이를 관리하는 규제와 감사 영역과 지원 인프라 영역에 대한 기회를 추가로 도출하는 탐색 기법이다. 분석 기회 발굴의 범위 확장 기업과 산업 환경을 중심으로 거시적 관점, 경쟁자, 시장의 니즈, 역량 등 4가지 영역에 대해 비즈니스 문제를 발굴하는 방법이다. 외부 참조 모델 기반 문제 탐색 ‘Quick&amp;Easy’ 방식으로 산업별, 업무 서비스별 분석과제 POOL을 만들고, 이를 브레인스토밍 형태로 탐색하여 비즈니스 문제를 빠르게 발굴하는 방법이다. 분석 유스케이스 현재의 유사 및 동종 사례를 탐색하여 분석 유스케이스를 찾아내고, 이를 바탕으로 비즈니스 문제를 발굴하는 방법이다. 분석 유스케이스란 분석을 적용했을 때 업무 흐름을 개념적으로 설명한 것으로 프로세스 혁신 수단으로 활용되기도 한다. B. 문제 정의 단계 : 앞서 1단계 문제 탐색 과정을 거쳐 비즈니스 문제가 식별되면 이를 데이터의 문제로 변환해야 한다. 즉, 데이터 분석에 관한 관점으로 전환하는 것이다. 예컨대 영업 부서에서 ‘최근 고객들의 불만이 높아지고 있다’는 비즈니스 문제가 식별됐다고 가정해 보자. 이를 데이터의 문제로 변환하면, ‘고객의 불만에 영향을 끼치는 요인이 무엇인지 분석하고, 그 요인과 고객 불만율에 대한 상관 및 예측 모델을 수립한다’ 정도로 바꿔볼 수 있다. 이것이 하향식 접근법의 2단계인 데이터로의 ‘문제 정의’ 단계다. C. 해결방안 탐색 단계 : 먼저 기존 시스템으로 가능한지와 기업 자체의 역량이 있는지를 판단하여 다음 그림과 같이 4개의 해결 방안 중 하나를 선정한다. D. 타당성 검토 : 하향식 접근법의 4번째 단계로 경제적 타당성과 데이터 및 기술적 타당성을 검토한다. 경제적 타당성 검토는 비용 대비 편익 분석 관점의 접근이 필요하며, 데이터 및 기술적 타당성 검토는 데이터 존재 여부, 분석 시스템 환경, 분석 역량 등에 대해 이루어져야 한다. 상향식 분석과제 발굴 방법론 문제 정의 자체가 어려운 경우 데이터를 기반으로 문제를 지속적으로 개선하는 방식 하향식 접근법의 한계를 극복하기 위한 분석 방법론으로써 디자인 사고 접근법을 사용하여 객관적인 데이터 그 자체를 관찰하고 실제적으로 행동에 옮겨 대상을 이해하는 방식을 적용함 가. 지도 · 비지도학습 나. 프로토타입 접근법(시행착오 해결법) : 상향식 접근법 중 하나로 시행착오 해결법이라고도 한다. 먼저 분석을 시도하고 그 결과를 확인하면서 조금씩 개선해나가는 방법. 포트폴리오 사분면(Quadrant) 분석을 활용한 분석 과제 우선순위 평가우선순위 평가 기준을 난이도와 시급성을 동시에 고려해 판단한다. 우선 추진해야 하는 분석 과제와 단기적 또는 중장기적으로 추진해야 하는 분석과제 등 4가지 유형으로 구분해 분석 과제의 적용 우선순위를 결정한다. 다음 표에서 우선순위 평가 기준을 ‘시급성’에 둔다면 ‘ⅲ → ⅳ → ⅱ → ⅰ’ 순서로, ‘난이도’에 둔다면 ‘ⅲ → ⅰ → ⅱ → ⅳ’ 순서로 우선순위를 정할 수 있다. 데이터 분석 과제 추진 시 고려해야 하는 우선순위 평가 기준 구분 설명 시급성 목표 가치와 전략적 중요도에 부합하는지에 따른 시급성이 가장 중요한 기준 난이도 현재 기업의 분석 수준과 데이터를 생성, 저장, 가공, 분석하는 비용을 고려한 난이도는 중요한 기준 분석 로드맵 수립 2. 분석 요건 정의분석 요건 정의 프로세스 3. 데이터 분석 모델링02_분석 작업 계획**학습목표** 데이터 분석 프로젝트를 기획하는 과정에 대해 학습한다. 전체의 흐름을 잘 이해하고 그다음에 암기하는 과정이 필요하다. 1. 데이터 확보 계획데이터 획득 방안 수립 내 · 외부의 다양한 시스템으로부터 정형&#x2F; 비정형&#x2F; 반정형 데이터를 수집하기 위한 구체적인 방안 수립 내부 데이터 획득에는 부서 간 업무 협조와 개인정보보호 및 정보보안에 관련된 문제점을 사전에 점검하고, 외부 데이터 획득은 시스템 간 다양한 인터페이스 및 법적인 문제점을 고려하여 상세한 데이터 획득 계획을 수립함. 데이터 확보 계획 수립 절차 단계 업무 내용 목표 정의 - 성과 목표 정의 - 성과 지표 설정 - 비즈니스 도메인 특성 적용 - 구체적인 성과목표 정의 - 성과측정을 위한 지표 도출 요구사항 도출 데이터 및 기술 지원 등과 관련된 요구사항 도출 - 필요 데이터 확보 및 관리 계획 - 데이터 정제 수준, 데이터 저장 형태 - 기존 시스템 및 도구 활용 여부 - 플랫폼 구축 여부 예산안 수립 자원 및 예산 수립 - 데이터 확보, 구축, 정비, 관리 예산 계획 수립 - 인력 투입 방안 - 일정 관리 - 위험 및 품질관리 - 프로젝트 관리 계획 수립 범위, 일정, 인력, 의사소통 방안 수립 | 2. NCS 기반 빅데이터 분석 절차 및 계획 수립NCS 기반 빅데이터 분석 절차 및 계획 수립의 흐름 빅데이터 분석 절차 절차 설명 문제 인식 - 비즈니스 문제와 기회를 인식하고 분석 목적을 정의 연구조사 - 목적 달성을 위한 각종 문헌을 조사 - 조사 내용을 해결방안에 적용 - 중요 변화요소 조사 모형화 - 복잡한 문제를 분리하고 단순화하는 과정 자료 수집 - 데이터 수집, 변수 측정 과정 자료 분석 - 수집된 자료에서 의미 찾기 분석결과 공유 - 변수 간의 관련성을 포함한 분석결과 제시 - 의사결정자와 결과 공유 - 표, 그림, 차트를 활용하여 가시화 빅데이터 분석 작업 WBS 설정 단계 내용 데이터 분석 과제 정의 - 분석 목표 정의서를 기준으로 프로젝트 전체 일정에 맞게 사전 준비를 하는 단계 데이터 준비 및 탐색 - 데이터 처리 엔지니어와 데이터 분석가의 역할을 구분하여 세부 일정이 만들어지는 단계 - 분석 목표 정의서에 기재된 내용을 중심으로 데이터 처리 엔지니어가 필요 데이터를 수집하고 정리하는 일정 수립 - 데이터 분석가가 분석에 필요한 데이터들로부터 변수 후보를 탐색하고 최종적으로 도출하는 일정 수립 데이터 분석 모델링 및 검증 - 데이터 준비 및 탐색이 완료된 이후 데이터 분석 가설이 증명된 내용을 중심으로 데이터 분석 모델링을 진행하는 단계 산출물 정리 - 데이터 분석 단계별 산출물을 정리하고, 분석 모델링 과정에서 개발된 분석 스크립트 등을 정리하여 최종 산출물로 정리하는 단계 출처 내용 출처 이지패스 2021 빅데이터분석기사 필기(수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음 https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 1. 빅데이터 분석 기획 (2) 빅데이터 분석 기획]- 세우초밥 https://blog.naver.com/ericalee97/222162993681","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"1과목. 빅데이터 분석 기획 1장. 빅데이터의 이해","slug":"Study/DataQ/BigData/Class1.1_Understanding_Big_Data","date":"2022-09-21T00:00:00.000Z","updated":"2022-09-26T08:44:28.821Z","comments":true,"path":"2022/09/21/Study/DataQ/BigData/Class1.1_Understanding_Big_Data/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/21/Study/DataQ/BigData/Class1.1_Understanding_Big_Data/","excerpt":"","text":"1장. 빅데이터의 이해 빅데이터를 공부할 때 가장 기초가 되는 부분이다. 핵심 키워드 빅데이터의 개념 데이터사이언티스트 인사이트 도출 빅데이터 플랫폼 01_빅데이터 개요 및 활용학습목표 빅데이터의 기초 개념과 산업 그리고 조직에 관한 일반적인 지식을 습득한다. 1. 빅데이터 기초데이터의 유형 정량적 데이터(정형 데이터) - 수치로 표현할 수 있는 숫자 · 도형 · 기호 등의 데이터 - 저장 · 검색 · 분석 활용에 용이 정성적 데이터(비정형 데이터) - 언어 · 문자 등의 정형화되지 않은 데이터 - ‘영화 감상평’, ‘SNS 실시간 검색어’ 등의 정성적 데이터 수집과 분석에는 상대적으로 많은 비용과 기술적 투자가 필요 암묵지와 형식지 암묵지(Tacit Knowledge) : 학습과 체험을 통해 개인에게 습득되어 있지만, 겉으로 드러나지 않는 상태의 지식 형식지(Explicit Knowledge) : 암묵지가 문서나 매뉴얼처럼 외부로 표출돼 여러 사람이 공유할 수 있는 지식 암묵지와 형식지의 상호작용 : 공유화되기 어려운 암묵지가 형식지로 표출되고 연결되면 그 상호작용으로 지식이 형성된다. 암묵지 형식지 공통화(Socialization) 표출화(Externalization) 내면화(Internalization) 연결화(Combination) 데이터와 정보 데이터(Data) : 개별 데이터 자체는 의미가 중요하지 않은 객관적인 사실 정보(Information) : 데이터의 가공 · 처리와 데이터 간 연관 관계 속에서 의미가 도출된 것 지식(Knowledge) : 데이터를 통해 도출된 다양한 정보를 구조화하여 유의미한 정보를 분류하고 개인적인 경험을 결합해 고유의 지식으로 내재화된 것 지혜(Wisdom) : 지식의 축적과 아이디어가 결합된 창의적 산물 데이터베이스의 특징 정보의 축적 및 전달 측면 - 기계 가독성 : 대량의 정보를 일정한 형식에 따라 컴퓨터 등의 정보처리기기가 읽고 쓸 수 있다. - 검색 가능성 : 다양한 방법으로 필요한 정보를 검색할 수 있다. - 원격 조작성 : 정보통신망을 통해 원거리에서도 즉시 온라인으로 이용 가능하다. 정보이용 측면 - 이용자의 정보요구에 따라 다양한 정보를 신속하게 획득할 수 있고 원하는 정보를 정확하고 경제적으로 찾아낼 수 있다. 정보관리 측면 - 정보를 일정한 질서와 구조에 따라 정리 · 저장하고 검색 · 관리할 수 있게 하여 방대한 양의 정보를 체계적으로 축적하고, 새로운 내용 추가나 갱신이 용이하다. 정보기술발전 측면 - 데이터베이스는 정보처리, 검색 · 관리 소프트웨어, 관련 하드웨어, 정보 전송을 위한 네트워크 기술 등의 발전을 견인할 수 있다. 경제 · 산업적측면 - 데이터베이스는 다양한 정보를 필요에 따라 신속하게 제공 · 이용할 수 있는 인프라의 특성을 가지고 있어 경제, 산업, 사회 활동의 효율성을 제고하고 국민의 편의를 증진하는 수단으로써의 의미를 가진다. 부문별 사회 기반 구조의 데이터베이스 물류 부문 - ‘실시간 차량 추적’을 위한 종합물류정보망 구축 - CVO 서비스, EDI 서비스, 물류 정보 DB 서비스, 부가서비스로 구성 - CALS(Commerce At Light Speed): 제품의 설계 · 개발 · 생산에서 유통 · 폐기에 이르기까지 제품의 라이프사이클 전반에 관련된 데이터를 통합하고 공유 · 교환할 수 있게 한 경영통합정보시스템을 말한다. - PORT-MIS : 항만운영정보시스템 - KROIS : 철도운영정보시스템 지리 부문 - GIS 응용에 활용하는 4S 통합기술 - 지리정보유통망 가시화 : 지리정보 통합관리소 운영, 지리정보 수요자에 정보 제공 - GIS(Geographic Information System) : 지리정보시스템 - LBS(Location-Based Service) : 위치정보서비스 - SIM(Spatial Information Management) : 공간정보 관리시스템 교통 부문 - 지능형교통정보시스템(ITS), 교통정보, 기초자료 및 통계 제공, 대국민 서비스 확대 의료 부문 - 의료정보시스템 : 처방전달시스템, 임상병리, 전자의무기록, 영상처리시스템, 병원의 멀티미디어, 원격의료, 지식 정보화 - HL7 국내 표준화 작업에 따라 전국적인 진료 정보 공유 체계 구축 계획 수립 - U헬스 실현에 기존 의료정보 데이터베이스 기반 활용 - PACS(Picture Archiving and Communications System) - U-Health(Ubiquitous-Health) 교육 부문 - 첨단 정보통신기술(ICT)을 활용한 각종 교육 정보의 개발 및 보급, 정보 활용 교육 - 대학 정보화 및 교육행정 정보화 위주로 사업 추진 - 교육행정정보시스템(NEIS)은 학사뿐만 아니라 기타 교육행정 전 업무를 처리하는 시스템 2. 빅데이터의 이해와 가치빅데이터의 특징 - 3V 구분 세부내용 Volume(크기) 대량의 데이터 증가 발생으로 기존 데이터 수집, 관리 한계 Variety(다양성) 비정형 데이터(영상, SNS 등)의 발생으로 다양한 데이터 형식 증가 Velocity(속도) 실시간 정보 발생으로 데이터의 유입, 처리 속도 요구 Value(가치) 데이터 전체를 파악하고 패턴을 발견하기가 어렵게 되면서 가치(Value)의 중요성 강조 Veracity(정확성) 빅데이터 기반의 예측 분석 결과에 대한 신뢰성이 중요하게 됨 빅데이터에 거는 기대 빅데이터는 “산업혁명의 석탄 · 철” 제조업뿐만 아니라 서비스 분야의 생산성을 획기적으로 끌어올려 사회 · 경제 · 문화 · 생활 전반에 혁명적 변화를 가져올 것으로 기대된다. 빅데이터는 “21세기 원유” 빅데이터도 원유처럼 각종 비즈니스, 공공기관 대국민 서비스, 그리고 경제 성장에 필요한 정보를 제공하여 산업 전반의 생산성을 향상시키고 새로운 범주의 산업을 만들어낼 것으로 기대된다. 빅데이터는 “렌즈” 렌즈를 통해 현미경이 생물학 발전에 끼쳤던 영향만큼, 빅데이터도 렌즈처럼 산업 발전에 큰 영향을 줄 것으로 기대된다. 대표 사례) 구글의 Ngram Viewer 빅데이터는 “플랫폼” 플랫폼은 공동 활용의 목적으로 구축된 유무형의 구조물을 말한다. 빅데이는 플랫폼으로서 다양한 서드파티 비즈니스에 활용될 것으로 기대된다. 대표 사례) 페이스북, 카카오톡 등 빅데이터가 만들어내는 변화 사전처리 → 사후처리 : 데이터를 사전 처리하지 않고, 가능한 많은 데이터를 모으고 데이터를 다양한 방식으로 조합하여 숨은 인사이트를 발굴한다. 표본조사 → 전수조사 : IoT · 클라우드 기술의 발전으로 데이터 처리 비용이 감소하게 되면서, 데이터 활용 방법이 표본조사에서 전수조사로 변화됐다. 질 → 양 : 수집 데이터의 양이 증가할수록 분석의 정확도가 높아져 양질의 분석 결과 산출에 긍정적인 영향을 줬다. 인과관계 → 상관관계 : 특정한 인과관계가 중요시되던 과거와 달리, 데이터의 양이 급격하게 늘어나면서 상관관계를 통해 특정 현상의 발생 가능성이 포착되고 그에 상응하는 행동을 추천하는 등 상관관계를 통한 인사이트 도출이 점점 확산되고 있다. 7가지 빅데이터 활용 기본 테크닉 테크닉 방법 예 연관규칙 학습(Association rule learning) 어떤 변인 간에 주목할 만한 상관관계가 있는지를 찾아내는 방법 - A를 구매한 사람이 B를 더 많이 사는가? - 이것을 구매한 사람들이 많이 구매한 물품은? - 장바구니 분석 - 상품 추천 유형 분석(Classification tree analysis) 새로운 사건이 속할 범주를 찾아 내는 일 - 이 사용자가 어떤 특성을 가진 집단에 속하는가? - 마케팅에서의 세그멘테이션(세분화) - 소비자 유형 분류 유전 알고리즘(Genetic algorithms) 최적화가 필요한 문제의 해결책을 자연 선택, 돌연변이 등과 같은 메커니즘을 통해 점진적으로 진화시키는 방법 - 최대 시청률을 얻으려면 어떤 프로그램을 어느 시간대에 방송할지 파악 - 최적화된 택배 차량 배치 기계 학습 &#x3D; 머신러닝(Machine learning) 학습 데이터로부터 학습한 알려진 특성을 활용해 ‘예측’하는 데 초점 - 기존 시청 기록을 바탕으로 시청자가 보유한 영화 중 어떤 영화를 가장 보고 싶어 하는지를 파악 - 넷플릭스의 영화 추천 시스템 회귀분석(Regression analysis) 독립변수를 조작하면서 종속변수가 어떻게 변하는지를 보며 두변인의 관계를 파악 - 구매자의 나이가 구매 차량의 타입에 어떤 영향을 미치는가? 감정분석(Sentiment analysis) 특정 주제에 대해 말하거나 글을 쓴 사람의 감정을 분석 - 새로운 환불 정책에 대한 고객의 평가는 어떤가? 소셜 네트워크 분석(Social network analysis) 오피니언 리더, 즉 영향력 있는 사람을 찾아낼 수 있으며, 고객 간 소셜 관계를 파악 - 특정인과 다른 사람이 몇 촌 정도의 관계인가? - 이 사람이 어느 정도 영향력 있는 인플루언서인가? 빅데이터의 위기 요인과 통제 방안 위기 요인 통제 방안 1. 사생활 침해 1. 동의에서 책임으로 - 개인의 사생활 침해 위협을 넘어 사회 · 경제적 위협으로 변형될 수 있음 - 익명화 기술이 발전되고 있으나, 아직 충분하지 않음. 정보가 오용될 때 위협의 크기는 막대함 - 예) 조지 오웰의 &lt;&lt;1984&gt;&gt;에서의 ‘빅브리더’ - 개인정보 제공자의 ‘동의’를 통해 해결하기보다 개인정보 사용자의 ‘책임’으로 해결 2. 책임 원칙 훼손 2. 결과 기반 책임 원칙 고수 - 빅데이터 기반 분석과 예측 기술이 발전하면서 정확도가 증가한 만큼, 분석 대상이 되는 사람들이 예측 알고리즘의 희생양이 될 가능성도 높아짐 - 빅데이터 시스템에 의해 부당하게 피해 보는 상황을 최소화할 장치 마련 필요 - 예) 영화 &lt;마이너리티 리포트&gt; - 특정인의 ‘성향’에 따라 처벌하는 것이 아닌 ‘행동 결과’를 보고 처벌 - 신용카드 발급 여부 판단에 있어 불이익을 배제 3. 데이터 오용 3. 알고리즘 접근 허용 - 데이터 과신, 잘못된 지표의 사용으로 인한 잘못된 인사이트를 얻어 비즈니스에 적용할 경우 직접 손실 발생 - 예) 적군의 사망자 수로 전쟁의 승리를 예측하는 오류 - 알고리즘 접근권 보장 - 알고리즈미스트 : 알고리즘에 의해 불이익을 당한 사람들을 대변해 피해자를 구제할 능력을 갖춘 전문가로서, 컴퓨터와 수학, 통계학이나 비즈니스에 두루 깊은 지식을 갖춘 전문가 대두 3. 빅데이터 산업의 이해데이터 사이언티스트에게 요구되는 역량 하드 스킬(Hard skill) 빅데이터에 대한 이론적 지식 : 관련 기법에 대한 이해와 방법론 습득 분석 기술에 대한 숙련 : 최적의 분석 설계 및 노하우 축적 소프트 스킬(Soft skill) 통찰력 있는 분석 : 창의적 사고, 호기심, 논리적 비판 설득력 있는 전달 : 스토리텔링, 시각화 다분야 간 협력 : 커뮤니케이션 가치 패러다임의 변화 디지털화(Digitalization) - 아날로그 세상을 어떻게 효과적으로 디지털화하는가가 이 시대의 가치를 창출해내는 원천 예) 도스 운영체제, 워드&#x2F;파워포인트와 같은 오피스 프로그램 연결(Connection) - 디지털화된 정보와 대상들이 서로 연결되어 이 연결이 얼마나 효과적이고 효율적으로 제공되느냐가 이 시대의 성패를 가름 예) 구글의 검색 알고리즘, 네이버의 콘텐츠 에이전시(Agency) - 사물인터넷(IoT)의 성숙과 함께 연결이 증가하고 복잡해짐 - 복잡한 연결을 얼마나 효과적이고 믿을 만하게 관리하는가가 이슈 - 데이터 사이언스의 역량에 따라 좌우 4. 빅데이터 조직 및 인력분석 준비도 평가 분석 업무 파악 분석 인력 및 조직 분석 기법 - 발생한 사실 분석 업무 - 예측 분석 업무 - 시뮬레이션 분석 업무 - 최적화 분석 업무 - 분석 업무 정기적 개선 - 분석전문가 직무 존재 - 분석전문가 교육 훈련프로그램 - 관리자 기본 분석 능력 - 전사총괄조직 - 경영진 분석 업무 이해 - 업무별 적합한 분석 기법 사용 - 분석 업무 도입 방법론 - 분석 기법 라이브러리 - 분석 기법 효과성 평가 - 분석 기법 정기적 개선 분석 데이터 분석 문화 IT 인프라 - 분석 업무를 위한 데이터 충분성&#x2F; 신뢰성&#x2F; 적시성 - 비구조적 데이터 관리 - 외부데이터 활용 체계 - 기준 데이터 관리(MDM) - 사실에 근거한 의사결정 - 관리자의 데이터 중시 - 회의 등에서 데이터 활용 - 경영진 직관보다 데이터 활용 - 데이터 공유 및 협업 문화 - 운영시스템 데이터 통합 - EAI, ETL 등 데이터 유통체계 - 분석 전용 서버 및 스토리지 - 빅데이터&#x2F; 통계&#x2F; 비주얼 분석환경 분석 성숙도 평가 단계 내용 비즈니스 부문 조직 · 역량 부문 IT 부문 [1단계] 도입 분석 시작, 환경과 시스템 구축 - 실적 분석 및 통계 - 정기 보고 수행 - 운영 데이터 기반 - 일부 부서에서 수행 - 담당자 역량에 의존 - 데이터 웨어하우스 - 데이터 마트 - ETL&#x2F; EAI - OLAP [2단계] 활용 분석 결과를 업무에 적용 - 미래결과 예측 - 시뮬레이션 - 운영 데이터 기반 - 전문 담당 부서 수행 - 분석 기법 도입 - 관리자가 분석 수행 - 실시간 대시보드 - 통계분석 환경 [3단계] 확산 전사 차원에서 분석 관리, 공유 - 전사성과 실시간 분석 - 프로세스 혁신 3.0 - 분석규칙 관리 - 이벤트 관리 - 전사 모든 부서 수행 - 분석 COE 운영 - 데이터 사이언티스트 확보 - 빅데이터 관리 환경 - 시뮬레이션 · 최적화 - 비주얼 분석 - 분석 전용 서버 [4단계] 최적화 분석을 진화시켜 혁신 및 성과 향상에 기여 - 외부 환경 분석 활용 - 최적화 업무 적용 - 실시간 분석 - 비즈니스 모델 진화 - 데이터 사이언스 그룹 - 경영진 분석 활용 - 전략 연계 - 분석 협업환경 - 분석 SandBox - 프로세스 내재화 - 빅데이터 분석 분석준비도 및 성숙도 진단 결과를 4분면으로 구분 분석 조직 유형 02_ 빅데이터 기술 및 제도학습목표 빅데이터 플랫폼의 개념과 빅데이터 에코시스템을 구성하고 있는 개별 요소들의 기능에 대해 이해한다. 인공지능과 마이데이터의 배경과 그 내용에 대해 알아본다. 1. 빅데이터 플랫폼빅데이터 플랫폼 개념도 에코시스템빅데이터는 수집, 정제, 적재, 분석, 시각화의 여러 단계를 거치는데, 이 단계를 거치는 동안 여러 가지 기술 및 프레임워크, 솔루션 등을 이용해 플랫폼을 완성하게 된다. 바로 이 기술 및 프레임워크, 솔루션 등을 통틀어 빅데이터 에코시스템(Big Data Ecosystem)이라고 한다. Google, IBM, Amazon, Oracle 등은 직접 개발한 여러 솔루션을 하나로 묶어 빅데이터 생태계, 즉 에코시스템을 구축하고 있다. 2. 빅데이터와 인공지능머신러닝과 딥러닝의 차이머신러닝은 데이터를 분석하고, 분석한 후에 그 데이터를 바탕으로 결정을 내리기 위해 학습한 내용을 적용하는 알고리즘을 말한다. 딥러닝은 카테고리 상으로는 머신러닝에 포함된 개념이지만, 실제로는 딥러닝 기술이 훨씬 더 진보적이다. 딥러닝은 예측의 정확성 여부를 스스로 판단하고 결정을 내린다. 딥러닝은 인간이 결론을 내리는 방식과 유사한 논리 구조로 데이터를 엄청나게 빠른 속도로 분석하여 결과를 도출해낸다. 이런 분석 기술을 인공신경망 분석 기술이라고 부른다. 미래의 인공지능 활용 분야 자율주행 - 우리나라 자동차 반자율주행에서 완전자율주행으로 기술 발전 - 자율주행차 신규 제조업체 등장 예상 - 자율주행 보험 및 여행 서비스 등장 스마트 홈 - 스마트 가전으로 편리한 생활 - 공간 지능화로 노인 케어 방식 변화 메디컬 케어 - 바이탈데이터 및 유전자 정보 활용 건강 관리 - 의료진 대상 지적 도우미 서비스 등장 - 우리나라 원격진료의 경우 관련법 개정 및 정책 지원이 필요 인프라 - 우리나라 공공 스마트 사업 본격화 - 전력, 수도, 가스 등의 자동 조절 - 다리, 발전소 등 공공인프라 이상 감지 스마트 농업 - 농업의 대형화 및 자동화 - 농작업 자동화 및 드론 이용 정밀 농업 - AI 기반 기상 예측 및 농업 보험 등장 업무 환경 변화 - 서류 관리 및 데이터 분석 등 자동화 전문 업무에 AI 도입 활발(법률, 번역) - 음성 인식 관련 기술 발전 및 관련 산업 확대 자율 배송, 유통 - 자율 화물배송, 무인화물선, 드론 배달 - 물류 창고 내 AI 로봇 이용 커머스 - 무인 점포, 얼굴 인식으로 추천 서비스 - 구매 데이터 분석으로 맞춤형 광고 제공 스마트 교육 - 학생별 맞춤형 교육 콘텐츠 제공 - AI 기반 대학 커리큘럼 설계 지원 AI 기반 금융 - 주식, 투자 상품의 로봇 어드바이저 신용 평가, 대출, 금융상품 지원 업무 안전 보장 로봇 - 재해 지역 구조 활동 - 극한 환경에서 자율 행동 가능 인텔리전스 시큐리티 - 빅데이터 분석에 따른 범죄 예측&#x2F;예방 - 행동 분석으로 이상 행동 사전 감지 3. 개인정보 활용 ‘마이데이터’‘마이데이터(Mydata)’ 운동의 확산마이데이터 운동은 정보의 주체가 개인정보 권한을 갖고 관리할 수 있게 하자는 취지다. 2015년 브뤼셀에서 처음 시작된 이 운동은 유럽을 거쳐 전 세계로 확산되었다. 마이뎅터 기구(mydata.org) 사이트에서 이들의 선언문(declaration)을 보면 그 취지를 잘 알 수 있다. 마이데이터 선언문에는 ‘이 선언에서 제시하는 변화와 원친은 균형을 회복하고 개인 정보에 대한 인간 중심의 비전을 향해 나아가는 것을 목표로 한다’고 명시되어 있다. 그리고 이것은 ‘공정하고 지속 가능하며 번영하는 디지털 사회의 조건’이라고 말한다. 마이데이터 개념 종전 ‘동의’ 제도와 마이데이터의 ‘동의’ 제도 업권별 마이데이터 서비스 예시 4. 개인정보보호 법 · 제도출처 내용 출처 이지패스 2021 빅데이터분석기사 필기 (수험서 앱 제공) &#x2F; 위키북스 &#x2F; 전용문, 정다혜, 임예은, 오경서 지음https://wikibook.co.kr/bigdata2021/ [빅데이터 분석기사 필기 | 1. 빅데이터 분석 기획 (1) 빅데이터의 이해] - 세우초밥https://blog.naver.com/ericalee97/222162052706 개인정보보호법 - 국가법령정보센터https://www.law.go.kr/법령/개인정보보호법 사진 출처 DIKW 피라미드 도식도: https://codedragon.tistory.com/9330","categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"}],"author":"엄태현"},{"title":"I am ~","slug":"About/eomtaehyeon","date":"2022-09-20T10:25:18.896Z","updated":"2022-09-16T08:20:40.000Z","comments":true,"path":"2022/09/20/About/eomtaehyeon/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/20/About/eomtaehyeon/","excerpt":"","text":"Bachelor of Food Culinary from Seoul Hoseo Vocational College majored in Bachelor of Food Culinary and Bachelor of Arts in Hotel Confectionery and Baking Learning R, SQL, and Python Interested in CV, NLP 1. 기본정보 생년월일 : 1993년 9월 15일 | 남 E-mail : &#x67;&#x6b;&#x64;&#x6c;&#114;&#104;&#x66;&#x6f;&#64;&#110;&#x61;&#x76;&#x65;&#114;&#46;&#x63;&#x6f;&#x6d; 휴대폰번호 : 010-7417-8917 주소 : (02459) 서울 동대문구 회기동 🎓학력 기타 교육 및 수료 2022.03.10 ~ 2022.08.30 : 휴먼교육센터학원 빅데이터전문가 수료 2018.12. (1주) : 일본과자전문학교 교육수료 2018.07. (2주) : 프랑스국립제과제빵학교 INBP 교육 수료 학교 2020.03. ~2021.08. : 서울호서직업전문학교 식품조리학과 학사학위 취득 2018.03. ~ 2021.02 : 서울호서직업전문학교 호텔제과제빵학과 전문학사 학위 취득 2012.02. : 대구 영남고등학교 졸업 🍀경력 서비스직 2020.02.25 ~ 2021.12.31 비알코리아 수도권직영팀 &#x2F; 사원(매니저) &#x2F; 2년차 (서울) 담당 업무 : 배스킨라빈스 서울 직영매장판매직(홍대상상마당, 왕십리역사, 청량리역사) 생산직 2017.08. ~ 2018.01. 엠디유글로벌(주) 케이크생산부 생산팀 &#x2F; 사원 &#x2F; 1년차 (대구) 담당업무 : 케이크 제품 시트, 냉동빵 생지, 쿠키, 제과류 생산 및 포장 2016.06. ~ 2016.12. 빠다롤더테라스 제과제빵 생산팀 &#x2F; 대리 &#x2F; 1년차 (대구) 제과제빵 반죽 및 소스 당당 🧾자격증&#x2F;면허증 기능사 2018.02.28 제과기능사 &#x2F; 18804590001D &#x2F; 한국산업인력공단 2018.01.31 제빵기능사 &#x2F; 18801580020G &#x2F; 한국산업인력공단 2014.09.25 정보처리기능사 &#x2F; 14832590066A &#x2F; 한국산업인력공단 면허증 2012.02.12 1종보통운전면허 &#x2F; 경찰청(운전면허시험관리단) 👩‍🚀경험 스터디 2022.04.10 ~ 2022.07.09 논문 스터디 : https://github.com/eomtaehyeon/Deep_Machine_Learning_Paper_Study 작업툴 : Notion, Github, Powerpoint, Visual Studio Code, Google Colab 인원 : 3명 (장민지, 엄태현 , 김영재) 딥러닝 및 머신러닝 모델에 대한 관련 논문을 읽고 블로그에 정리 딥러닝 및 머신러닝 모델을 코드로 구현 및 실습 2022.03.10 ~ 2022.08.30 캐글데이터분석 스터디 : https://github.com/eomtaehyeon/Kaggle 작업 툴 : Kaggle, VS Code, Google Colab, R Studio, Notion, Powerpoint, Google Cloud, MySQL 사용한 코딩 언어 : Python, R, JAVA 인원 : 5명 (오승은, 강지원, 장민지, 김영재, 엄태현) Kaggle을 통해 빅데이터 분석을 공부합니다. Kaggle을 통해 Machine Learning&#x2F; Deep Learning의 모델을 공부합니다. Kaggle을 통해 데이터 전처리 과정을 공부하고 EDA의 개념을 정리하고 확립합니다. 아르바이트 2022.03. / 2022.08. 배스킨라빈스 홍대상상마당점&#x2F; 서초우성점 주말마감파트너 2019.11.25 ~ 2019.12.31 나무와벽돌 12월 연말 및 크리스마스 시즌 제과 제빵생산 및 지원 2018.08. ~ 2019.11. 주식회사 해차 음식점에서 서비스 업무와 주방보조 담당 해외 2019.07.03 ~ 2019.07.26 르바카 사브레 제과점 일본교토 르바카 사브레 제과점에서 산학연수 교내활동 2018.03. ~ 2020.12. 서울호서직업전문학교 서울호서직업전문학교 호텔제과제빵학과 실습조교 및 반대표 활동 봉사활동 2018.10.12 ~ 2018.10.14 강서구 허준축제 강서구 허준근린공원에서 아이싱쿠키부스 운영 병역 해군병장만기제대(699일) 2013.12. ~ 2014.12.06 해군교육사령부 &#x2F; 행정병 2013.03. ~ 2013.12. 포항항만방어기지 &#x2F; 항만고속정 &#x2F; 갑판병 2013.01.07 ~ 2013.03. 해군교육사령부 &#x2F; 훈련병 포트폴리오 데이터 분석 및 개발 포트폴리오 2022.07.14 ~ 2022.08.26 머신러닝을 이용한 자바 프로젝트 베르데테르 2022.05.20 ~ 2020.06.03 부동산 금리계산기 서비스 챗봇 프로젝트 ZIPFLIX프로젝트 2022.04.01 ~ 2022.05.01 Kaggle Competition을 통한 시계열 데이터 분류 모델 연구 시계열활용사례 그 외 포트폴리오 2019.03.01 ~ 2022.10.31 호텔제과제빵과 졸업작품전 - 하늘연달 플리마켓 졸업작품전","categories":[{"name":"About","slug":"About","permalink":"http://eomtaehyeon.github.io/categories/About/"}],"tags":[]},{"title":"Hexo블로그 테마적용하기","slug":"Github/Hexo/HexoThemeSetting","date":"2022-09-16T00:00:00.000Z","updated":"2022-09-16T06:11:36.000Z","comments":true,"path":"2022/09/16/Github/Hexo/HexoThemeSetting/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/16/Github/Hexo/HexoThemeSetting/","excerpt":"","text":"개요 Hexo 프레임워크의 테마적용하여 블로그 꾸미기. Themes | Hexo hexo-theme-icarus : https://ppoffice.github.io/hexo-theme-icarus/ hexo-theme-tranquilpeak : https://github.com/LouisBarranqueiro/hexo-theme-tranquilpeak hexo-theme-hueman : https://github.com/ppoffice/hexo-theme-hueman Hexo 추천테마, Hueman 적용.설치하기 blog 폴더에서 명령어로 테마를 설치. 1$ git clone https://github.com/ppoffice/hexo-theme-hueman.git themes/hueman blog의 _config.yml 을 수정. 1theme: huuman themes/hueman 폴더 안에 있는 _config.yml.example 의 이름을 **_config.yml**로 수정. hueman 테마에서 제공하는 검색 기능 1$ npm install -S hexo-generator-json-content 커스터마이징 themes/hueman 폴더 안에 있는 _config.yml 파일을 수정하면 간단하게 커스터마이징이 가능 123456789101112131415161718192021# Customizecustomize: logo: width: 165 height: 60 url: images/logo-header.png theme_color: &#x27;#e79d4c&#x27; highlight: androidstudio sidebar: &#x27;left&#x27; # sidebar position, options: left, right thumbnail: false # enable posts thumbnail, options: true, false favicon: # path to favicon social_links: # for more icons, please see http://fontawesome.io/icons/#brand user: https://github.com/eomtaehyeon/Resume instagram: https://www.instagram.com/hi__gorae/ github: https://github.com/eomtaehyeon # twitter: / # stack-overflow: / # weibo: / # rss: / .md 파일에 **Title**과 Category 설정 123456---title: 정적 사이트 생성date: &#x27;2022-09-16&#x27;category: [Settings,Blog]tags: [Github,Blog,Hexo]--- Reference Futurecreator - Hexo추천테마, hueman 적용하기 : https://futurecreator.github.io/2016/06/14/hexo-apply-hueman-theme/","categories":[{"name":"Settings","slug":"Settings","permalink":"http://eomtaehyeon.github.io/categories/Settings/"},{"name":"Blog","slug":"Settings/Blog","permalink":"http://eomtaehyeon.github.io/categories/Settings/Blog/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://eomtaehyeon.github.io/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"http://eomtaehyeon.github.io/tags/Blog/"},{"name":"Hexo","slug":"Hexo","permalink":"http://eomtaehyeon.github.io/tags/Hexo/"}]},{"title":"정적 사이트 생성","slug":"Github/Hexo/hexoBlog","date":"2022-09-16T00:00:00.000Z","updated":"2022-09-16T06:02:26.000Z","comments":true,"path":"2022/09/16/Github/Hexo/hexoBlog/","link":"","permalink":"http://eomtaehyeon.github.io/2022/09/16/Github/Hexo/hexoBlog/","excerpt":"","text":"개요 정적 사이트를 만들만한 프레임워크들을 조사하기. Hexo 프레임워크를 이용하여 블로그를 만들어 관리하기. 정적 사이트 프레임워크Hexo Javascript(Node.js)기반 사용하는 사람 다수.(Reference 다수) Git으로 포스트 버전관리가 어렵다. Hugo Golang 기반으로 속도가 빠르다. Jekyll 가장 많은 사용자 보유. Ruby 기반으로 속도가 느리다. Github페이지에서 Jekyll 사용하여 Hexo의 단점을 상쇄. Hexo 블로그 프레임워크1. 필수 파일 설치 1단계 : Nodejs.org 다운로드 설치가 완료되었다면 간단하게 확인가능. 1$ node -v 2단계 : git-scm.com 다운로드 설치가 완료되었다면 간단하게 확인가능. 1$ git --version 3단계 : hexo 설치 hexo는 npm을 통해서 설치가 가능하다. 1$ npm install -g hexo-cli 2. Github 설정 두개의 깃허브 Repostories 를 생성. 포스트 버전관리(name : blog) 포스트 배포용 관리(name : eomtaehyeon.github.io) eomtaehyeon 대신에 각자의 username 을 입력하면 된다. 3. 블로그 만들기 리눅스 기본명령어 123mkdir blog # blog라는 이름을 가진 폴더를 만든다.cd blog # blog 폴더로 이동.cd .. # 현재 폴더에서 상위 폴더로 이동. hexo init “디렉토리” 123456789$ hexo init blogINFO Cloning hexo-starter https://github.com/hexojs/hexo-starter.gitINFO Install dependenciesnpm WARN config global `--global`, `--local` are deprecated. Use `--location=global` instead.INFO Start blogging with Hexo!$ cd blog$ npm install$ npm install hexo-server --save$ npm install hexo-deployer-git --save 에러구문. ERROR Deployer not found: git hexo-deployer-git을 설치 하지 않으면 deploy시 위와 같은 ERROR가 발생합니다. _config.yml 파일 설정 블로그 사이트 정보 수정 1234title: 제목을 지어주세요subtitle: 부제목을 지어주세요description: description을 지어주세요author: YourName 블로그 URL 정보 설정 1234url: https://eomtaehyeon.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults: Github 연동 12345# Deploymentdeploy: type: git repo: https://github.com/eomtaehyeon/eomtaehyeon.github.io.git branch: main 4. Github에 배포하기 배포 하기 전, 확인하기 1234$ hexo generate$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 화면 확인이 된 이후 깃허브에 배포 배포하지 않을 파일 : gitignore 파일에서 설정 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 최종적으로 배포 1$ hexo deploy • 배포가 완료가 되면 브라우저에서 eomtaehyeon.github.io로 접속해 정상적으로 배포가 되었는지 확인한다. Reference Hexo Blog 만들기 - DSCHLOE : https://dschloe.github.io/settings/hexo_blog&#x2F; Hexo Docs : https://hexo.io/ko/docs/ Hexo를 이용한 블로그 만들기 - jihyehwang09 : https://jihyehwang09.github.io/2019/01/03/hexo-blog/","categories":[{"name":"Settings","slug":"Settings","permalink":"http://eomtaehyeon.github.io/categories/Settings/"},{"name":"Blog","slug":"Settings/Blog","permalink":"http://eomtaehyeon.github.io/categories/Settings/Blog/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://eomtaehyeon.github.io/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"http://eomtaehyeon.github.io/tags/Blog/"},{"name":"Hexo","slug":"Hexo","permalink":"http://eomtaehyeon.github.io/tags/Hexo/"}]}],"categories":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"자격증공부요점정리/빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/categories/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"논문리뷰","slug":"논문리뷰","permalink":"http://eomtaehyeon.github.io/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"},{"name":"NLP","slug":"논문리뷰/NLP","permalink":"http://eomtaehyeon.github.io/categories/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/NLP/"},{"name":"About","slug":"About","permalink":"http://eomtaehyeon.github.io/categories/About/"},{"name":"Settings","slug":"Settings","permalink":"http://eomtaehyeon.github.io/categories/Settings/"},{"name":"Blog","slug":"Settings/Blog","permalink":"http://eomtaehyeon.github.io/categories/Settings/Blog/"}],"tags":[{"name":"자격증공부요점정리","slug":"자격증공부요점정리","permalink":"http://eomtaehyeon.github.io/tags/%EC%9E%90%EA%B2%A9%EC%A6%9D%EA%B3%B5%EB%B6%80%EC%9A%94%EC%A0%90%EC%A0%95%EB%A6%AC/"},{"name":"빅데이터분석기사필기","slug":"빅데이터분석기사필기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EA%B8%B0%EC%82%AC%ED%95%84%EA%B8%B0/"},{"name":"빅분기","slug":"빅분기","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%B6%84%EA%B8%B0/"},{"name":"필기","slug":"필기","permalink":"http://eomtaehyeon.github.io/tags/%ED%95%84%EA%B8%B0/"},{"name":"빅데이터","slug":"빅데이터","permalink":"http://eomtaehyeon.github.io/tags/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"데이터사이언티스트","slug":"데이터사이언티스트","permalink":"http://eomtaehyeon.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%ED%8B%B0%EC%8A%A4%ED%8A%B8/"},{"name":"논문리뷰","slug":"논문리뷰","permalink":"http://eomtaehyeon.github.io/tags/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/"},{"name":"NLP","slug":"NLP","permalink":"http://eomtaehyeon.github.io/tags/NLP/"},{"name":"NAACL","slug":"NAACL","permalink":"http://eomtaehyeon.github.io/tags/NAACL/"},{"name":"BERT","slug":"BERT","permalink":"http://eomtaehyeon.github.io/tags/BERT/"},{"name":"Transformers","slug":"Transformers","permalink":"http://eomtaehyeon.github.io/tags/Transformers/"},{"name":"Github","slug":"Github","permalink":"http://eomtaehyeon.github.io/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"http://eomtaehyeon.github.io/tags/Blog/"},{"name":"Hexo","slug":"Hexo","permalink":"http://eomtaehyeon.github.io/tags/Hexo/"}]}